{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from os.path import exists\n",
    "\n",
    "def infer_column_types(df):\n",
    "    for col in df.columns:\n",
    "        try:\n",
    "            unique_values = df[col].dropna().unique()\n",
    "        except:\n",
    "            df[col] = df[col].apply(lambda x: json.dumps(x))\n",
    "            unique_values = df[col].dropna().unique()\n",
    "        \n",
    "        if set(unique_values).issubset({\"True\", \"False\", \"0\", \"1\"}):\n",
    "            df[col] = df[col].map(lambda x: True if x in [\"True\", \"1\"] else False).astype(\"bool\")\n",
    "        elif np.all(~pd.isna(pd.to_numeric(unique_values, errors=\"coerce\"))): \n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\", downcast=\"integer\")\n",
    "        elif df[col].nunique() / len(df) < 0.1:\n",
    "            df[col] = df[col].astype(\"string\").astype(\"category\")\n",
    "\n",
    "os.makedirs(f\"helm_tables\", exist_ok=True)\n",
    "BENCHMARKS = [\"classic\", \"thaiexam\", \"mmlu\", \"lite\"]\n",
    "lo = lambda x: json.load(open(x, \"r\"))\n",
    "\n",
    "task2metric = lo(\"task2metric.json\")\n",
    "task2metric = pd.json_normalize(task2metric)\n",
    "\n",
    "all_paths = []\n",
    "for benchmark in BENCHMARKS:\n",
    "    dir_path = f\"helm_jsons/{benchmark}/releases\"\n",
    "    assert exists(dir_path)\n",
    "    latest_release = sorted(os.listdir(dir_path))[-1]\n",
    "    folder_dict = lo(f\"{dir_path}/{latest_release}/runs_to_run_suites.json\")\n",
    "    all_paths += [f\"helm_jsons/{benchmark}/runs/{s}/{r}\" for r, s in folder_dict.items()]\n",
    "\n",
    "files = [\"display_requests.json\", \"display_predictions.json\", \"run_spec.json\"]\n",
    "all_paths = [p for p in tqdm(all_paths) if all([exists(f\"{p}/{f}\") for f in files])]\n",
    "all_lists = [[lo(f\"{p}/{f}\") for p in tqdm(all_paths)] for f in files]\n",
    "\n",
    "results = []\n",
    "for d_requests, d_predictions, run_specs, paths in tqdm(zip(*all_lists, all_paths), total=len(all_lists[0])):\n",
    "    d_requests = pd.json_normalize(d_requests)\n",
    "    d_predictions = pd.json_normalize(d_predictions)\n",
    "    run_specs = pd.json_normalize(run_specs)\n",
    "    benchmark = paths.split(\"/\")[1]\n",
    "    run_specs[\"benchmark\"] = benchmark\n",
    "    run_specs = run_specs.loc[run_specs.index.repeat(d_predictions.shape[0])].reset_index(drop=True)\n",
    "    overlap_column = d_predictions.columns.intersection(d_requests.columns)\n",
    "    d_requests = d_requests.drop(columns=overlap_column)\n",
    "    result = pd.concat([d_requests, d_predictions, run_specs], axis=1)\n",
    "    result[\"scenario\"] = result['name'].str.split(r'[:,]', n=1, expand=True)[0]\n",
    "    result[\"scenario\"] = result[\"scenario\"].astype(\"category\")\n",
    "    assert result[\"scenario\"].nunique() == 1\n",
    "    metric_name = task2metric[f\"{benchmark}.{result['scenario'].iloc[0]}\"].iloc[0]\n",
    "    if isinstance(metric_name, list):\n",
    "        for metric_name_ in metric_name:\n",
    "            dicho_score = result.get(f\"stats.{metric_name_}\", pd.NA)\n",
    "            if dicho_score is not pd.NA:\n",
    "                if not dicho_score.isna().all():\n",
    "                    result[\"dicho_score\"] = dicho_score\n",
    "                    break\n",
    "    else:\n",
    "        result[\"dicho_score\"] = result.get(f\"stats.{metric_name}\", pd.NA)\n",
    "    \n",
    "    result = result[[\"request.model\", \"request.prompt\", \"scenario\", \"dicho_score\"]]\n",
    "    results.append(result)\n",
    "\n",
    "results = pd.concat(results, axis=0, join='outer')\n",
    "print(\"finished create results dataframe\")\n",
    "infer_column_types(results)\n",
    "results.reset_index(drop=True, inplace=True)\n",
    "for col in results.columns:\n",
    "    if results[col].dtype != \"category\" and np.isnan(results[col]).all():\n",
    "        results = results.drop(columns=col)\n",
    "    else:\n",
    "        if results[col].dtype == \"float64\" and np.nanmax(results[col]) < 65500 and np.nanmin(results[col]) > -65500:\n",
    "            results[col] = results[col].astype(\"float16\")\n",
    "print(\"Started saving results\")\n",
    "results.to_pickle(\"helm_tables/responses.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[\"request.model\"] = results[\"request.model\"].astype(\"category\")\n",
    "results[\"request.prompt\"] = results[\"request.prompt\"].astype(\"category\")\n",
    "results[\"scenario\"] = results[\"scenario\"].astype(\"category\")\n",
    "results[\"dicho_score\"] = results[\"dicho_score\"].astype(\"float16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[dup_keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6512249\n"
     ]
    }
   ],
   "source": [
    "dup_keys = results.duplicated(keep=False)\n",
    "print(dup_keys.sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AlephAlpha/luminous-base'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[dup_keys].iloc[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>request.model</th>\n",
       "      <th>request.prompt</th>\n",
       "      <th>scenario</th>\n",
       "      <th>dicho_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AlephAlpha/luminous-base</td>\n",
       "      <td>Passage: Mice are afraid of cats.\\nSheep are a...</td>\n",
       "      <td>babi_qa</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>AlephAlpha/luminous-base</td>\n",
       "      <td>Passage: Mice are afraid of cats.\\nSheep are a...</td>\n",
       "      <td>babi_qa</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AlephAlpha/luminous-extended</td>\n",
       "      <td>Passage: Mice are afraid of cats.\\nSheep are a...</td>\n",
       "      <td>babi_qa</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>AlephAlpha/luminous-extended</td>\n",
       "      <td>Passage: Mice are afraid of cats.\\nSheep are a...</td>\n",
       "      <td>babi_qa</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AlephAlpha/luminous-supreme</td>\n",
       "      <td>Passage: Mice are afraid of cats.\\nSheep are a...</td>\n",
       "      <td>babi_qa</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    request.model  \\\n",
       "0        AlephAlpha/luminous-base   \n",
       "610      AlephAlpha/luminous-base   \n",
       "0    AlephAlpha/luminous-extended   \n",
       "610  AlephAlpha/luminous-extended   \n",
       "0     AlephAlpha/luminous-supreme   \n",
       "\n",
       "                                        request.prompt scenario  dicho_score  \n",
       "0    Passage: Mice are afraid of cats.\\nSheep are a...  babi_qa          1.0  \n",
       "610  Passage: Mice are afraid of cats.\\nSheep are a...  babi_qa          1.0  \n",
       "0    Passage: Mice are afraid of cats.\\nSheep are a...  babi_qa          0.0  \n",
       "610  Passage: Mice are afraid of cats.\\nSheep are a...  babi_qa          0.0  \n",
       "0    Passage: Mice are afraid of cats.\\nSheep are a...  babi_qa          0.0  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = results[results[\"request.prompt\"] == results[dup_keys].iloc[0, 1]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"request.model\"] == results[dup_keys].iloc[0, 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>request.model</th>\n",
       "      <th>request.prompt</th>\n",
       "      <th>scenario</th>\n",
       "      <th>dicho_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AlephAlpha/luminous-base</td>\n",
       "      <td>Passage: Mice are afraid of cats.\\nSheep are a...</td>\n",
       "      <td>babi_qa</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>AlephAlpha/luminous-base</td>\n",
       "      <td>Passage: Mice are afraid of cats.\\nSheep are a...</td>\n",
       "      <td>babi_qa</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                request.model  \\\n",
       "0    AlephAlpha/luminous-base   \n",
       "610  AlephAlpha/luminous-base   \n",
       "\n",
       "                                        request.prompt scenario  dicho_score  \n",
       "0    Passage: Mice are afraid of cats.\\nSheep are a...  babi_qa          1.0  \n",
       "610  Passage: Mice are afraid of cats.\\nSheep are a...  babi_qa          1.0  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Passage: Mice are afraid of cats.\\n'\n",
      " 'Sheep are afraid of cats.\\n'\n",
      " 'Emily is a sheep.\\n'\n",
      " 'Winona is a mouse.\\n'\n",
      " 'Wolves are afraid of cats.\\n'\n",
      " 'Gertrude is a wolf.\\n'\n",
      " 'Jessica is a wolf.\\n'\n",
      " 'Cats are afraid of mice.\\n'\n",
      " 'Question: What is winona afraid of?\\n'\n",
      " 'Answer: cat\\n'\n",
      " '\\n'\n",
      " 'Passage: Wolves are afraid of sheep.\\n'\n",
      " 'Sheep are afraid of mice.\\n'\n",
      " 'Winona is a wolf.\\n'\n",
      " 'Cats are afraid of mice.\\n'\n",
      " 'Mice are afraid of sheep.\\n'\n",
      " 'Jessica is a sheep.\\n'\n",
      " 'Emily is a cat.\\n'\n",
      " 'Gertrude is a cat.\\n'\n",
      " 'Question: What is gertrude afraid of?\\n'\n",
      " 'Answer: mouse\\n'\n",
      " '\\n'\n",
      " 'Passage: Wolves are afraid of cats.\\n'\n",
      " 'Cats are afraid of sheep.\\n'\n",
      " 'Mice are afraid of cats.\\n'\n",
      " 'Gertrude is a cat.\\n'\n",
      " 'Winona is a mouse.\\n'\n",
      " 'Sheep are afraid of cats.\\n'\n",
      " 'Jessica is a wolf.\\n'\n",
      " 'Emily is a cat.\\n'\n",
      " 'Question: What is emily afraid of?\\n'\n",
      " 'Answer: sheep\\n'\n",
      " '\\n'\n",
      " 'Passage: Wolves are afraid of mice.\\n'\n",
      " 'Cats are afraid of wolves.\\n'\n",
      " 'Winona is a wolf.\\n'\n",
      " 'Emily is a wolf.\\n'\n",
      " 'Sheep are afraid of wolves.\\n'\n",
      " 'Mice are afraid of wolves.\\n'\n",
      " 'Jessica is a mouse.\\n'\n",
      " 'Gertrude is a cat.\\n'\n",
      " 'Question: What is jessica afraid of?\\n'\n",
      " 'Answer: wolf\\n'\n",
      " '\\n'\n",
      " 'Passage: Cats are afraid of mice.\\n'\n",
      " 'Wolves are afraid of sheep.\\n'\n",
      " 'Gertrude is a wolf.\\n'\n",
      " 'Sheep are afraid of cats.\\n'\n",
      " 'Emily is a wolf.\\n'\n",
      " 'Jessica is a sheep.\\n'\n",
      " 'Mice are afraid of cats.\\n'\n",
      " 'Winona is a sheep.\\n'\n",
      " 'Question: What is winona afraid of?\\n'\n",
      " 'Answer: cat\\n'\n",
      " '\\n'\n",
      " 'Passage: Sheep are afraid of mice.\\n'\n",
      " 'Cats are afraid of mice.\\n'\n",
      " 'Jessica is a sheep.\\n'\n",
      " 'Wolves are afraid of mice.\\n'\n",
      " 'Mice are afraid of wolves.\\n'\n",
      " 'Emily is a wolf.\\n'\n",
      " 'Gertrude is a wolf.\\n'\n",
      " 'Winona is a mouse.\\n'\n",
      " 'Question: What is emily afraid of?\\n'\n",
      " 'Answer:')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(results[dup_keys].iloc[0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "\n",
    "def find_common_prefix(strings):\n",
    "    \"\"\"Find the longest common prefix shared by all strings in a group.\"\"\"\n",
    "    if not strings: return \"\"\n",
    "    common_prefix = strings[0]\n",
    "    for s in strings[1:]:\n",
    "        while not s.startswith(common_prefix) and common_prefix:\n",
    "            common_prefix = common_prefix[:-1]\n",
    "    return common_prefix\n",
    "\n",
    "def detect_subgroups(strings, threshold=0.6):\n",
    "    \"\"\"\n",
    "    Group strings such that each subgroup's common prefix (updated as strings are added)\n",
    "    has a length that is at least `threshold` fraction of the shorter string compared.\n",
    "    \"\"\"\n",
    "    if not strings: return []\n",
    "    \n",
    "    sorted_strings = sorted(strings)\n",
    "    clusters = []\n",
    "    current_cluster = [sorted_strings[0]]\n",
    "    current_prefix = sorted_strings[0]\n",
    "\n",
    "    for s in sorted_strings[1:]:\n",
    "        new_common_prefix = find_common_prefix([current_prefix, s])\n",
    "        min_length = min(len(current_prefix), len(s))\n",
    "        ratio = len(new_common_prefix) / min_length if min_length > 0 else 0\n",
    "        \n",
    "        if ratio >= threshold:\n",
    "            current_cluster.append(s)\n",
    "            current_prefix = new_common_prefix\n",
    "        else:\n",
    "            clusters.append(current_cluster)\n",
    "            current_cluster = [s]\n",
    "            current_prefix = s\n",
    "    clusters.append(current_cluster)\n",
    "    return clusters\n",
    "\n",
    "def create_batches(prompts, token_lengths, max_tokens_per_batch):\n",
    "    \"\"\"\n",
    "    Create batches such that each batch contains at most max_batch_size prompts\n",
    "    and the total token count in the batch does not exceed max_tokens_per_batch.\n",
    "    Prompts that individually exceed max_tokens_per_batch are skipped.\n",
    "    \"\"\"\n",
    "    batches = []\n",
    "    current_batch = []\n",
    "    current_tokens = 0\n",
    "\n",
    "    for prompt, token_count in tqdm(zip(prompts, token_lengths), total=len(prompts)):\n",
    "        if current_batch and current_tokens + token_count > max_tokens_per_batch:\n",
    "            batches.append(current_batch)\n",
    "            current_batch = []\n",
    "            current_tokens = 0\n",
    "\n",
    "        current_batch.append(prompt)\n",
    "        current_tokens += token_count\n",
    "\n",
    "    if current_batch: batches.append(current_batch)\n",
    "    return batches\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "model_nickname = model_name.split(\"/\")[-1]\n",
    "checkpoint = \"results_perplexity_forthattempt.pkl\"\n",
    "max_token_length = 2048\n",
    "# export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 in code\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\n",
    "vllm_model = LLM(\n",
    "    model_name,\n",
    "    gpu_memory_utilization=0.9,\n",
    "    enable_chunked_prefill=False,\n",
    "    enforce_eager=True,\n",
    "    dtype=torch.float16,\n",
    "    swap_space=32,\n",
    "    max_num_seqs=128,\n",
    "    tensor_parallel_size=len(os.environ[\"CUDA_VISIBLE_DEVICES\"].split(\",\")),\n",
    ")\n",
    "tokenizer = vllm_model.get_tokenizer()\n",
    "sampling_params = SamplingParams(n=1, temperature=1, max_tokens=1, prompt_logprobs=0)\n",
    "\n",
    "if os.path.exists(checkpoint):\n",
    "    results = pd.read_pickle(checkpoint)\n",
    "    batch_start = int(open(checkpoint + \".batch_count\").read())\n",
    "else:\n",
    "    results = pd.read_pickle(\"../gather_helm_data/helm_tables/responses.pkl\")\n",
    "    for group in tqdm(results[\"groups\"].unique()):\n",
    "        # Preserve order while removing duplicates\n",
    "        example_strings = results.loc[results[\"groups\"] == group, \"request.prompt\"].to_list()\n",
    "        unique_example_strings = []\n",
    "        seen = set()\n",
    "        for s in example_strings:\n",
    "            if s not in seen:\n",
    "                unique_example_strings.append(s)\n",
    "                seen.add(s)\n",
    "        \n",
    "        subgroups = detect_subgroups(unique_example_strings, threshold=0.8)\n",
    "        mapping = {}\n",
    "        mapping_i = {}\n",
    "        for subgroup in subgroups:\n",
    "            if len(subgroup) == 1:\n",
    "                # Singleton subgroup: no common prefix exists, so don't strip anything.\n",
    "                s = subgroup[0]\n",
    "                mapping[s] = s\n",
    "                mapping_i[s] = 0\n",
    "            else:\n",
    "                common_prefix = find_common_prefix(subgroup)\n",
    "                for s in subgroup:\n",
    "                    mapping[s] = s[len(common_prefix):].strip()\n",
    "                    mapping_i[s] = len(common_prefix)\n",
    "        \n",
    "        # Create a boolean mask for the current group and assign mapped values\n",
    "        mask = results[\"groups\"] == group\n",
    "        results.loc[mask, \"question_start_index\"] = results.loc[mask, \"request.prompt\"].map(mapping_i)\n",
    "    \n",
    "    results[f\"perplexity_{model_nickname}\"] = np.nan\n",
    "    results[\"question_start_index\"] = results[\"question_start_index\"].astype(int)\n",
    "    batch_start = 0\n",
    "\n",
    "    unique_prompts = list(results[\"request.prompt\"].unique())\n",
    "    encoded = tokenizer(unique_prompts, add_special_tokens=False)\n",
    "    lengths = [len(ids) for ids in encoded[\"input_ids\"]]\n",
    "    unique_prompt_to_length = dict(zip(unique_prompts, lengths))\n",
    "    results[\"token_length\"] = results[\"request.prompt\"].map(unique_prompt_to_length)\n",
    "\n",
    "    # Precompute the token index corresponding to the question portion.\n",
    "    # This is computed as the number of tokens in the substring up to the question_start_index.\n",
    "    unique_mapping = results[[\"request.prompt\", \"question_start_index\"]].drop_duplicates(subset=\"request.prompt\")\n",
    "    prompt_to_q_index = dict(zip(unique_mapping[\"request.prompt\"], unique_mapping[\"question_start_index\"]))\n",
    "\n",
    "    # For each unique prompt, compute its token index (number of tokens in prompt[:question_start_index])\n",
    "    question_token_indices = { \n",
    "        prompt: len(tokenizer.encode(prompt[:q_index])) \n",
    "        for prompt, q_index in prompt_to_q_index.items() \n",
    "    }\n",
    "    results[\"question_token_index\"] = results[\"request.prompt\"].map(question_token_indices)\n",
    "\n",
    "    results.to_pickle(checkpoint)\n",
    "    with open(checkpoint + \".batch_count\", \"w\") as f:\n",
    "        f.write(str(batch_start))\n",
    "\n",
    "results = results.dropna(subset=[\"dicho_score\"])\n",
    "filtered = results.loc[results[\"token_length\"] < 2048, [\"request.prompt\", \"token_length\"]]\n",
    "filtered = filtered.drop_duplicates(subset=\"request.prompt\")\n",
    "filtered = filtered.sort_values(\"token_length\", ascending=False)\n",
    "unique_prompts = filtered[\"request.prompt\"].tolist()\n",
    "unique_token_lengths = filtered[\"token_length\"].tolist()\n",
    "\n",
    "print(\"Start creating batches\")\n",
    "batches = create_batches(unique_prompts, unique_token_lengths, max_token_length)\n",
    "existing = results.loc[pd.notna(results[f\"perplexity_{model_nickname}\"]), [\"request.prompt\", f\"perplexity_{model_nickname}\"]]\n",
    "prompt_to_perp = dict(zip(existing[\"request.prompt\"], existing[f\"perplexity_{model_nickname}\"]))\n",
    "\n",
    "unique_mapping = results[[\"request.prompt\", \"question_token_index\"]].drop_duplicates(subset=\"request.prompt\")\n",
    "question_token_indices = dict(zip(unique_mapping[\"request.prompt\"], unique_mapping[\"question_token_index\"]))\n",
    "\n",
    "print(f\"fraction of filtered prompts: {len(unique_prompts) / len(unique_mapping):.2%}\")\n",
    "\n",
    "skip_count = 0\n",
    "for i, batch_prompts in tqdm(enumerate(batches[batch_start:]), total=len(batches[batch_start:])):\n",
    "    batch_outputs = vllm_model.generate(batch_prompts, sampling_params, use_tqdm=False)\n",
    "    for prompt, output in zip(batch_prompts, batch_outputs):\n",
    "        token_index = question_token_indices[prompt] #1\n",
    "        logprobs_list = output.prompt_logprobs[token_index:]\n",
    "        target_logprobs = [list(logprobs.values())[0].logprob for logprobs in logprobs_list]\n",
    "        if target_logprobs: prompt_to_perp[prompt] = np.exp(-np.mean(target_logprobs))\n",
    "        else: skip_count += 1\n",
    "\n",
    "    if (i > 0 and i % 10000 == 0) or i == len(batches) - 1:\n",
    "        print(f\"progress {i} / {len(batches)}, skipped {skip_count} items so far\")\n",
    "        results[f\"perplexity_{model_nickname}\"] = results[\"request.prompt\"].map(prompt_to_perp).astype(float)\n",
    "        results.to_pickle(checkpoint)\n",
    "        with open(checkpoint + \".batch_count\", \"w\") as f:\n",
    "            f.write(str(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from vllm import LLM\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "# List of GPU ids to use (adjust based on your available GPUs)\n",
    "gpu_ids = [0,1,2,4,6,7]  # This gives you 5 models (update if needed)\n",
    "num_models = len(gpu_ids)\n",
    "# model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "model_name = \"Alibaba-NLP/gte-Qwen2-7B-instruct\"\n",
    "# model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "\n",
    "def embed_sub_batch(gpu_id, sub_batch):\n",
    "    # Reinitialize the model in this process on the specified GPU.\n",
    "    llm = LLM(\n",
    "        model=model_name,\n",
    "        task=\"embed\", \n",
    "        gpu_memory_utilization=0.9,\n",
    "        # enable_chunked_prefill=False,\n",
    "        # enforce_eager=True,\n",
    "        tensor_parallel_size=1,\n",
    "        # tokenizer_pool_size=8,\n",
    "        device=f\"cuda:{gpu_id}\",\n",
    "    )\n",
    "    outputs = llm.embed(sub_batch)\n",
    "    # Extract and return the embedding from each output object.\n",
    "    return [o.outputs.embedding for o in outputs]\n",
    "\n",
    "results = pd.read_pickle(\"results_perplexity_forthattempt.pkl\")\n",
    "results = results.dropna(subset=[\"dicho_score\"])\n",
    "filtered = results.loc[results[\"token_length\"] < 2048, [\"request.prompt\", \"token_length\"]]\n",
    "filtered = filtered.drop_duplicates(subset=\"request.prompt\")\n",
    "filtered = filtered.sort_values(\"token_length\", ascending=False)\n",
    "unique_prompts = filtered[\"request.prompt\"].tolist()\n",
    "\n",
    "outputs = []\n",
    "batch_size = len(unique_prompts)\n",
    "\n",
    "for i in tqdm(range(0, len(unique_prompts), batch_size)):\n",
    "    batch = unique_prompts[i : i + batch_size]\n",
    "    # Split the batch into sub-batches for each GPU.\n",
    "    # This slicing method works even if len(batch) isn’t divisible by num_models.\n",
    "    sub_batches = [batch[j::num_models] for j in range(num_models)]\n",
    "    \n",
    "    futures = []\n",
    "    with ProcessPoolExecutor(max_workers=num_models) as executor:\n",
    "        for gpu_id, sub_batch in zip(gpu_ids, sub_batches):\n",
    "            if sub_batch:  # Only submit if there’s work.\n",
    "                futures.append(executor.submit(embed_sub_batch, gpu_id, sub_batch))\n",
    "        for future in as_completed(futures):\n",
    "            outputs.extend(future.result())\n",
    "    \n",
    "    # Optionally, save partial results to disk.\n",
    "    question_embedding = pd.DataFrame({\n",
    "        \"question\": unique_prompts[:len(outputs)],\n",
    "        \"embedding\": outputs\n",
    "    })\n",
    "    # question_embedding.to_pickle(f\"unique_prompts_embeddings_Mistral-7B-Instruct-v0.3.pkl\")\n",
    "    question_embedding.to_pickle(f\"unique_prompts_embeddings_gte-Qwen2-7B-instruct.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reeval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
