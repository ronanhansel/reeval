DESCRIPTION_MAP = {
    "air-bench/air_bench_2024": "### DATASET: AirBench, ### PUBLISH TIME: 2024, ### CONTENT: AI safety benchmark that aligns with emerging government regulations and company policies",
    "classic/babi_qa": "### DATASET: bAbI, ### PUBLISH TIME: 2015, ### CONTENT: for measuring understanding and reasoning",
    "classic/bbq": "### DATASET: BBQ (Bias Benchmark for Question Answering), ### PUBLISH TIME: 2022, ### CONTENT: for measuring social bias in question answering in ambiguous and unambigous context",
    # "classic/blimp": "",
    "classic/bold": "### DATASET: BOLD (Bias in Open-Ended Language Generation Dataset), ### PUBLISH TIME: 2021, ### CONTENT: for measuring biases and toxicity in open-ended language generation",
    "classic/boolq": "### DATASET: boolq, ### PUBLISH TIME: 2019, ### CONTENT: binary (yes/no) question answering, passages from Wikipedia, questions from search queries",
    "classic/civil_comments": "### DATASET: CivilComments, ### PUBLISH TIME: 2019, ### CONTENT: for toxicity detection",
    "classic/code": "### DATASET: Code, ### PUBLISH TIME: 2021, ### CONTENT: for measuring competence on code challenges, for measuring functional correctness for synthesizing programs from docstrings",
    "classic/commonsense": "### DATASET: HellaSwag, ### PUBLISH TIME: 2019, ### CONTENT: commonsense reasoning in question answering",
    # "classic/copyright": "",
    # "classic/disinfo": "",
    "classic/dyck_language_np=3": "### DATASET: Dyck, ### PUBLISH TIME: 2019, ### CONTENT: Scenario testing hierarchical reasoning through the Dyck formal languages",
    "classic/entity_data_imputation": "### DATASET: Data imputation, ### PUBLISH TIME: 2021, ### CONTENT: tests the ability to impute missing entities in a data table",
    "classic/entity_matching": "### DATASET: Entity matching, ### PUBLISH TIME: 2016, ### CONTENT: tests the ability to determine if two entities match",
    "classic/gsm": "### DATASET: GSM8K (Grade school math word problems), ### PUBLISH TIME: 2021, ### CONTENT: for testing mathematical reasoning on grade-school math problems",
    # "classic/ice": "",
    "classic/imdb": "### DATASET: IMDB, ### PUBLISH TIME: 2011, ### CONTENT: sentiment analysis in movie review",
    "classic/legal_support": "### DATASET: LegalSupport, ### PUBLISH TIME: unknown, ### CONTENT: measure fine-grained legal reasoning through reverse entailment.",
    "classic/lsat_qa": "### DATASET: LSAT, ### PUBLISH TIME: 2021, ### CONTENT: for measuring analytical reasoning on the Law School Admission Test",
    "classic/math": "### DATASET: MATH, ### PUBLISH TIME: 2021, ### CONTENT: for measuring mathematical problem solving on competition math problems with or without with chain-of-thought style reasoning",
    "classic/mmlu": "### DATASET: MMLU (Massive Multitask Language Understanding), ### PUBLISH TIME: 2021, ### CONTENT: for knowledge-intensive question answering across 57 domains",
    "classic/msmarco": "### DATASET: MSMARCO, ### PUBLISH TIME: 2016, ### CONTENT: for passage retrieval in information retrieval",
    "classic/narrative_qa": "### DATASET: NarrativeQA, ### PUBLISH TIME: 2017, ### CONTENT: for reading comprehension over narratives, passages are books and movie scripts",
    # "classic/natural_qa": "",
    "classic/quac": "### DATASET: QuAC (Question Answering in Context), ### PUBLISH TIME: 2018, ### CONTENT: question answering in the context of dialogues",
    "classic/raft": "### DATASET: RAFT (Real-world Annotated Few-Shot), ### PUBLISH TIME: 2021, ### CONTENT: meta-benchmark of 11 real-world text classification tasks",
    "classic/real_toxicity_prompts": "### DATASET: RealToxicityPrompts, ### PUBLISH TIME: 2020, ### CONTENT: for measuring toxicity in prompted model generations",
    # "classic/summarization_cnndm": "",
    "classic/summarization_xsum": "### DATASET: XSUM, ### PUBLISH TIME: 2018, ### CONTENT: for text summarization of BBC news articles",
    # 'classic/synthetic_efficiency': '### DATASET: Synthetic efficiency, ### PUBLISH TIME: unknown, ### CONTENT: to better understand inference runtime performance of various models',
    "classic/synthetic_reasoning": "### DATASET: Synthetic reasoning, ### PUBLISH TIME: 2021, ### CONTENT: defined using abstract symbols based on LIME and simple natural language based on LIME",
    "classic/synthetic_reasoning_natural": "### DATASET: Synthetic reasoning (natural language), ### PUBLISH TIME: 2021, ### CONTENT: Synthetic reasoning tasks defined using simple natural language based on LIME",
    # "classic/the_pile": "",
    "classic/truthful_qa": "### DATASET: TruthfulQA, ### PUBLISH TIME: 2022, ### CONTENT: for measuring model truthfulness and commonsense knowledge in question answering",
    "classic/twitter_aae": "### DATASET: TwitterAAE, ### PUBLISH TIME: 2016, ### CONTENT: for measuring language model performance in tweets as a function of speaker dialect, on African-American-aligned Tweets, on White-aligned Tweets",
    "classic/wikifact": "### DATASET: WikiFact, ### PUBLISH TIME: 2019, ### CONTENT: knowledge base completion, entity-relation-entity triples in natural language form, to more extensively test factual knowledge",
    "thaiexam/thai_exam": "### DATASET: Thai exam, ### PUBLISH TIME: 2024, ### CONTENT: a Thai language benchmark based on examinations for high school students and investment professionals in Thailand",
}
DATASETS = list(DESCRIPTION_MAP.keys()) + ["combined_data"]

SHORT_NAME_MAPPING = {
    "air-bench/air_bench_2024": "airbench",
    "classic/babi_qa": "babi_qa",
    "classic/bbq": "bbq",
    "classic/bold": "bold",
    "classic/boolq": "boolq",
    "classic/civil_comments": "civil_comments",
    "classic/code": "code",
    "classic/commonsense": "commonsense",
    "classic/dyck_language_np=3": "dyck_language_np3",
    "classic/entity_data_imputation": "entity_data_imputation",
    "classic/entity_matching": "entity_match",
    "classic/gsm": "gsm",
    "classic/imdb": "imdb",
    "classic/legal_support": "legal_support",
    "classic/lsat_qa": "lsat_qa",
    "classic/math": "math",
    "classic/mmlu": "mmlu",
    "classic/msmarco": "msmarco",
    "classic/narrative_qa": "narrative_qa",
    "classic/quac": "quac",
    "classic/raft": "raft",
    "classic/real_toxicity_prompts": "real_toxicity_prompts",
    "classic/summarization_xsum": "summarization_xsum",
    "classic/synthetic_reasoning": "synthetic_reasoning",
    "classic/synthetic_reasoning_natural": "synthetic_reasoning_natural",
    "classic/truthful_qa": "truthful_qa",
    "classic/twitter_aae": "twitter_aae",
    "classic/wikifact": "wikifact",
    "thaiexam/thai_exam": "thai_exam",
}

LONG_NAME_MAPPING = {v: k for k, v in SHORT_NAME_MAPPING.items()}

HELM_MODEL_MAP = {
    "text-davinci-002": "openai_text-davinci-002",
    "text-babbage-001": "openai_text-babbage-001",
    "ada (350M)": "openai_ada",
    "text-ada-001": "openai_text-ada-001",
    "babbage (1.3B)": "openai_babbage",
    "T0pp (11B)â˜ ": "together_t0pp",
    "text-davinci-003": "openai_text-davinci-003",
    "text-curie-001": "openai_text-curie-001",
    "davinci (175B)": "openai_davinci",
    "curie (6.7B)": "openai_curie",
}
