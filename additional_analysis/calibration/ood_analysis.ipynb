{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary cache directory at /tmp/user/21130/matplotlib-vb4dkkrr because the default path (/afs/cs.stanford.edu/u/sttruong/.cache/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n",
      "/tmp/user/21130/ipykernel_2487275/3141485721.py:36: FutureWarning: The default value of observed=False is deprecated and will change to observed=True in a future version of pandas. Specify observed=False to silence this warning and retain the current behavior\n",
      "  results = results.pivot_table(index=\"request.model\", columns=[\"instance_id\", \"groups\"], values=\"dicho_score\", aggfunc=majority_vote)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done pivoting\n",
      "0.6929338169796397\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tueplots import bundles\n",
    "from tqdm import tqdm\n",
    "bundles.icml2024()\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "def majority_vote(series):\n",
    "    # Filter out -1 values (no value)\n",
    "    valid = series[series != -1]\n",
    "    # If there are no valid values, return -1 (or handle as needed)\n",
    "    if len(valid) == 0:\n",
    "        return -1\n",
    "    # Count the number of 0's and 1's\n",
    "    count0 = (valid == 0).sum()\n",
    "    count1 = (valid == 1).sum()\n",
    "    # Return the majority; break ties arbitrarily\n",
    "    if count1 > count0:\n",
    "        return 1\n",
    "    elif count0 > count1:\n",
    "        return 0\n",
    "    else:\n",
    "        return random.choice([0, 1])\n",
    "\n",
    "# results = pd.read_pickle(\"../gather_helm_data/helm_tables/responses.pkl\")\n",
    "# results_full = pd.read_pickle(\"results_perplexity.pkl\")\n",
    "# results_full = pd.read_pickle(\"results_perplexity_thirdattempt.pkl\")\n",
    "results_full = pd.read_pickle(\"results_perplexity_forthattempt.pkl\")\n",
    "results = results_full[[\"request.model\", \"instance_id\", \"dicho_score\", \"groups\"]]\n",
    "results = results.dropna(subset=[\"dicho_score\"])\n",
    "results[\"dicho_score\"] = results[\"dicho_score\"].apply(\n",
    "    lambda x: x.item() if hasattr(x, \"item\") else x\n",
    ") # covert numpy.float16 to float\n",
    "results = results.pivot_table(index=\"request.model\", columns=[\"instance_id\", \"groups\"], values=\"dicho_score\", aggfunc=majority_vote)\n",
    "\n",
    "print(\"done pivoting\")\n",
    "\n",
    "# sort the columns by groups\n",
    "results = results.sort_index(axis=1, level=\"groups\")\n",
    "\n",
    "results = results.loc[:, (results != 0).any()]\n",
    "results = results.loc[:, (results != 1).any()]\n",
    "results = results.fillna(-1).astype(int)\n",
    "# Replace -1 with NaN so that missing scores are ignored\n",
    "results = results.replace(-1, np.nan)\n",
    "\n",
    "# Compute the overall average for each group manually\n",
    "group_means = {}\n",
    "for group in results.columns.get_level_values(\"groups\").unique():\n",
    "    mask = results.columns.get_level_values(\"groups\") == group\n",
    "    values = results.loc[:, mask].values  # all values for this group\n",
    "    group_means[group] = np.nanmean(values)\n",
    "\n",
    "# Sort the groups by their average score\n",
    "sorted_groups = sorted(group_means, key=group_means.get)\n",
    "\n",
    "# Create a mapping from group to its sort order\n",
    "group_order = {group: order for order, group in enumerate(sorted_groups)}\n",
    "\n",
    "# Reorder the columns based on the new group order using the key parameter\n",
    "results = results.sort_index(axis=1, level=\"groups\", key=lambda x: x.map(group_order))\n",
    "\n",
    "# Compute the overall average for each row (ignoring NaNs)\n",
    "row_means = results.mean(axis=1)\n",
    "\n",
    "# Sort the rows by these computed averages (lowest to highest)\n",
    "results = results.loc[row_means.sort_values().index]\n",
    "\n",
    "# convert nan back to -1\n",
    "results = results.replace(np.nan, -1)\n",
    "# count the fraction of -1 \n",
    "print((results == -1).sum().sum() / (results.shape[0] * results.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-05 10:47:23 arg_utils.py:1197] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.\n",
      "WARNING 03-05 10:47:23 cuda.py:95] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "WARNING 03-05 10:47:23 config.py:685] Async output processing is not supported on the current platform type cuda.\n",
      "INFO 03-05 10:47:23 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=PoolerConfig(pooling_type=None, normalize=None, softmax=None, step_tag_id=None, returned_token_ids=None), compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}, use_cached_outputs=False, \n",
      "INFO 03-05 10:47:26 model_runner.py:1110] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "INFO 03-05 10:47:27 weight_utils.py:254] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c594ef33502842afb64f2093602415ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-05 10:47:30 model_runner.py:1115] Loading model weights took 14.9576 GB\n",
      "finished loading model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:22<00:00, 22.03s/it]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from vllm import LLM\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "llm = LLM(\n",
    "    model=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    task=\"embed\", \n",
    "    gpu_memory_utilization=0.9,\n",
    "    enable_chunked_prefill=False,\n",
    "    enforce_eager=True,\n",
    "    # dtype=torch.float16,\n",
    "    # swap_space=32,\n",
    "    # max_num_seqs=128,\n",
    "    tensor_parallel_size=1,\n",
    "    tokenizer_pool_size=8,\n",
    ")\n",
    "print(\"finished loading model\")\n",
    "results = pd.read_pickle(\"results_perplexity_forthattempt.pkl\")\n",
    "results = results.dropna(subset=[\"dicho_score\"])\n",
    "filtered = results.loc[results[\"token_length\"] < 2048, [\"request.prompt\", \"token_length\"]]\n",
    "filtered = filtered.drop_duplicates(subset=\"request.prompt\")\n",
    "filtered = filtered.sort_values(\"token_length\", ascending=False)\n",
    "unique_prompts = filtered[\"request.prompt\"].tolist()\n",
    "\n",
    "# process the prompt in batch\n",
    "outputs = []\n",
    "batch_size = 128 # 20480\n",
    "# for i in tqdm(range(0, len(unique_prompts), batch_size)):\n",
    "for i in tqdm(range(0, batch_size, batch_size)):\n",
    "    batch = unique_prompts[i:i+batch_size]\n",
    "    output = llm.embed(batch, use_tqdm=False)\n",
    "    outputs.extend([o.outputs.embedding for o in output])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(i+1)*batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 128 out of 1023604 prompts.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Processed {i+len(batch)} out of {len(unique_prompts)} prompts.\") \n",
    "question_embedding = pd.DataFrame({\"question\": unique_prompts[:(i+1)*batch_size], \"embedding\": outputs})\n",
    "question_embedding.to_pickle(\"unique_prompts_embeddings.pkl\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reeval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
