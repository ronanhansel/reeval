scikit-learn
torch==2.4.0
datasets
transformers
numpy
pandas
wandb
matplotlib
tueplots
selenium
seaborn
python-dotenv
webdriver_manager
# flash-attn: only install on Linux x86_64 with CPython 3.10 where this wheel applies.
# On Apple Silicon / other platforms PyTorch's built-in scaled_dot_product_attention will be used instead.
flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.5.8/flash_attn-2.5.8+cu122torch2.3cxx11abiFALSE-cp310-cp310-linux_x86_64.whl ; \
	platform_system == "Linux" and platform_machine == "x86_64" and python_version == "3.10"
# If you want an alternative speedup on non-Linux platforms you can optionally install xformers (may have limited MPS support):
# xformers ; platform_system != "Linux" and python_version >= "3.10"
vllm ; platform_system == "Linux" and platform_machine == "x86_64" and python_version == "3.10"
torchmetrics
trl==0.12.1
fastparquet
statsmodels