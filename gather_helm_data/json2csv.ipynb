{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from os.path import exists\n",
    "\n",
    "def infer_column_types(df):\n",
    "    for col in df.columns:\n",
    "        try:\n",
    "            unique_values = df[col].dropna().unique()\n",
    "        except:\n",
    "            df[col] = df[col].apply(lambda x: json.dumps(x))\n",
    "            unique_values = df[col].dropna().unique()\n",
    "        \n",
    "        if set(unique_values).issubset({\"True\", \"False\", \"0\", \"1\"}):\n",
    "            df[col] = df[col].map(lambda x: True if x in [\"True\", \"1\"] else False).astype(\"bool\")\n",
    "        elif np.all(~pd.isna(pd.to_numeric(unique_values, errors=\"coerce\"))): \n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\", downcast=\"integer\")\n",
    "        elif df[col].nunique() / len(df) < 0.1:\n",
    "            df[col] = df[col].astype(\"string\").astype(\"category\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15816/15816 [00:00<00:00, 109936.68it/s]\n",
      "100%|██████████| 15815/15815 [07:37<00:00, 34.60it/s]  \n",
      "100%|██████████| 15815/15815 [02:14<00:00, 117.65it/s] \n",
      "100%|██████████| 15815/15815 [00:00<00:00, 29187.57it/s]\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(f\"helm_tables\", exist_ok=True)\n",
    "BENCHMARKS = [\"air-bench\", \"classic\", \"thaiexam\", \"mmlu\", \"lite\"]\n",
    "lo = lambda x: json.load(open(x, \"r\"))\n",
    "\n",
    "task2metric = lo(\"task2metric.json\")\n",
    "task2metric = pd.json_normalize(task2metric)\n",
    "\n",
    "all_paths = []\n",
    "for benchmark in BENCHMARKS:\n",
    "    dir_path = f\"helm_jsons/{benchmark}/releases\"\n",
    "    assert exists(dir_path)\n",
    "    latest_release = sorted(os.listdir(dir_path))[-1]\n",
    "    folder_dict = lo(f\"{dir_path}/{latest_release}/runs_to_run_suites.json\")\n",
    "    all_paths += [f\"helm_jsons/{benchmark}/runs/{s}/{r}\" for r, s in folder_dict.items()]\n",
    "\n",
    "files = [\"display_requests.json\", \"display_predictions.json\", \"run_spec.json\"]\n",
    "all_paths = [p for p in tqdm(all_paths) if all([exists(f\"{p}/{f}\") for f in files])]\n",
    "all_lists = [[lo(f\"{p}/{f}\") for p in tqdm(all_paths)] for f in files]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15815/15815 [11:37<00:00, 22.69it/s]  \n",
      "/tmp/user/21130/ipykernel_2666996/2641024592.py:27: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results = pd.concat(results, axis=0, join='outer')\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for d_requests, d_predictions, run_specs, paths in tqdm(zip(*all_lists, all_paths), total=len(all_lists[0])):\n",
    "    d_requests = pd.json_normalize(d_requests)\n",
    "    d_predictions = pd.json_normalize(d_predictions)\n",
    "    run_specs = pd.json_normalize(run_specs)\n",
    "    benchmark = paths.split(\"/\")[1]\n",
    "    run_specs[\"benchmark\"] = benchmark\n",
    "    run_specs = run_specs.loc[run_specs.index.repeat(d_predictions.shape[0])].reset_index(drop=True)\n",
    "    overlap_column = d_predictions.columns.intersection(d_requests.columns)\n",
    "    d_requests = d_requests.drop(columns=overlap_column)\n",
    "    result = pd.concat([d_requests, d_predictions, run_specs], axis=1)\n",
    "    result[\"scenario\"] = result['name'].str.split(r'[:,]', n=1, expand=True)[0]\n",
    "    result[\"scenario\"] = result[\"scenario\"].astype(\"category\")\n",
    "    assert result[\"scenario\"].nunique() == 1\n",
    "    metric_name = task2metric[f\"{benchmark}.{result['scenario'].iloc[0]}\"].iloc[0]\n",
    "    if isinstance(metric_name, list):\n",
    "        for metric_name_ in metric_name:\n",
    "            dicho_score = result.get(f\"stats.{metric_name_}\", pd.NA)\n",
    "            if dicho_score is not pd.NA:\n",
    "                if not dicho_score.isna().all():\n",
    "                    result[\"dicho_score\"] = dicho_score\n",
    "                    break\n",
    "    else:\n",
    "        result[\"dicho_score\"] = result.get(f\"stats.{metric_name}\", pd.NA)\n",
    "    results.append(result)\n",
    "\n",
    "results = pd.concat(results, axis=0, join='outer')\n",
    "infer_column_types(results)\n",
    "results.reset_index(drop=True, inplace=True)\n",
    "for col in results.columns:\n",
    "    if results[col].dtype != \"category\" and np.isnan(results[col]).all():\n",
    "        results = results.drop(columns=col)\n",
    "    else:\n",
    "        if results[col].dtype == \"float64\" and np.nanmax(results[col]) < 65500 and np.nanmin(results[col]) > -65500:\n",
    "            results[col] = results[col].astype(\"float16\")\n",
    "results.to_pickle(\"helm_tables/responses.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reeval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
