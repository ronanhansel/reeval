{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "from datetime import datetime\n",
        "from typing import List, Optional\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def _summarize_tensor(t: torch.Tensor):\n",
        "    t_cpu = t.detach().float().cpu()\n",
        "    flat = t_cpu.view(-1)\n",
        "    return {\n",
        "        \"shape\": list(t_cpu.shape),\n",
        "        \"numel\": int(flat.numel()),\n",
        "        \"mean\": float(flat.mean().item()) if flat.numel() > 0 else 0.0,\n",
        "        \"std\": float(flat.std(unbiased=False).item()) if flat.numel() > 1 else 0.0,\n",
        "        \"min\": float(flat.min().item()) if flat.numel() > 0 else 0.0,\n",
        "        \"max\": float(flat.max().item()) if flat.numel() > 0 else 0.0,\n",
        "        \"l2norm\": float(torch.norm(flat, p=2).item()),\n",
        "    }\n",
        "\n",
        "\n",
        "def trainer(\n",
        "    parameters: List[torch.Tensor],\n",
        "    optim,\n",
        "    closure,\n",
        "    n_iter: int = 100,\n",
        "    verbose: bool = True,\n",
        "    log_json_path: Optional[str] = None,\n",
        "    parameter_names: Optional[List[str]] = None,\n",
        "    log_every: int = 1,\n",
        "    save_full_parameters: bool = True,\n",
        "):\n",
        "    \"\"\"\n",
        "    Optimizer stepping loop with early stopping and per-iteration JSON logging.\n",
        "\n",
        "    - parameters: list of tensors to optimize\n",
        "    - optim: torch optimizer (e.g., LBFGS)\n",
        "    - closure: callable that returns loss and sets gradients\n",
        "    - n_iter: maximum outer iterations\n",
        "    - verbose: if True, shows tqdm progress\n",
        "    - log_json_path: if None, auto-creates logs/<timestamp>_<label>.jsonl\n",
        "    - parameter_names: optional names (same length as parameters)\n",
        "    - log_every: write one record every N iterations\n",
        "    - save_full_parameters: if True, writes full parameter values; otherwise writes summaries\n",
        "    \"\"\"\n",
        "    if parameter_names is None:\n",
        "        parameter_names = [f\"param_{i}\" for i in range(len(parameters))]\n",
        "\n",
        "    # Determine log destination\n",
        "    if log_json_path is None:\n",
        "        logs_dir = os.environ.get(\"TRAINER_LOG_DIR\", os.path.join(os.getcwd(), \"logs\"))\n",
        "        os.makedirs(logs_dir, exist_ok=True)\n",
        "        label = \"_\".join(parameter_names) if parameter_names else \"params\"\n",
        "        log_json_path = os.path.join(\n",
        "            logs_dir,\n",
        "            f\"{datetime.utcnow().strftime('%Y%m%dT%H%M%SZ')}_trainer_{label}.jsonl\",\n",
        "        )\n",
        "    else:\n",
        "        os.makedirs(os.path.dirname(log_json_path) or \".\", exist_ok=True)\n",
        "\n",
        "    pbar = tqdm(range(n_iter)) if verbose else range(n_iter)\n",
        "    previous_parameters = None\n",
        "    previous_loss = None\n",
        "\n",
        "    for iteration in pbar:\n",
        "        if iteration > 0:\n",
        "            previous_parameters = [p.clone() for p in parameters]\n",
        "            previous_loss = loss.clone() if 'loss' in locals() else None\n",
        "\n",
        "        loss = optim.step(closure)\n",
        "\n",
        "        # Compute deltas and grad norm for early stopping and logging\n",
        "        if iteration > 0 and previous_parameters is not None and previous_loss is not None:\n",
        "            d_loss = (previous_loss - loss).item()\n",
        "            d_parameters = sum(\n",
        "                torch.norm(prev - curr, p=2).item()\n",
        "                for prev, curr in zip(previous_parameters, parameters)\n",
        "            )\n",
        "        else:\n",
        "            d_loss = None\n",
        "            d_parameters = None\n",
        "\n",
        "        grad_norm = sum(\n",
        "            torch.norm(p.grad, p=2).item() for p in parameters if getattr(p, \"grad\", None) is not None\n",
        "        )\n",
        "\n",
        "        # Logging\n",
        "        if (iteration % log_every) == 0:\n",
        "            record = {\n",
        "                \"iteration\": int(iteration),\n",
        "                \"loss\": float(loss.item()),\n",
        "                \"grad_norm\": float(grad_norm),\n",
        "            }\n",
        "\n",
        "            if d_loss is not None:\n",
        "                record[\"d_loss\"] = float(d_loss)\n",
        "            if d_parameters is not None:\n",
        "                record[\"d_parameters\"] = float(d_parameters)\n",
        "\n",
        "            if save_full_parameters:\n",
        "                record[\"parameters\"] = {\n",
        "                    name: p.detach().float().cpu().tolist() for name, p in zip(parameter_names, parameters)\n",
        "                }\n",
        "            else:\n",
        "                record[\"parameter_summary\"] = {\n",
        "                    name: _summarize_tensor(p) for name, p in zip(parameter_names, parameters)\n",
        "                }\n",
        "\n",
        "            with open(log_json_path, \"a\") as fp:\n",
        "                fp.write(json.dumps(record) + \"\\n\")\n",
        "\n",
        "        if verbose and isinstance(pbar, tqdm):\n",
        "            pbar.set_postfix(\n",
        "                {\n",
        "                    \"grad_norm\": grad_norm,\n",
        "                    \"d_param\": d_parameters if d_parameters is not None else 0.0,\n",
        "                    \"d_loss\": d_loss if d_loss is not None else 0.0,\n",
        "                    \"loss\": float(loss.item()),\n",
        "                }\n",
        "            )\n",
        "\n",
        "        # Early stopping\n",
        "        if (\n",
        "            d_loss is not None\n",
        "            and d_parameters is not None\n",
        "            and abs(d_loss) < 1e-5\n",
        "            and d_parameters < 1e-5\n",
        "            and grad_norm < 1e-5\n",
        "        ):\n",
        "            break\n",
        "\n",
        "    return parameters\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Define functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "editable": true,
        "run_control": {
          "frozen": false
        }
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import pickle\n",
        "import os\n",
        "import json\n",
        "import gc\n",
        "from torch.distributions import Bernoulli\n",
        "from torch.optim import LBFGS\n",
        "from tqdm import tqdm\n",
        "from scipy.stats import pearsonr\n",
        "from collections import defaultdict\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "from multiprocessing import Manager\n",
        "import multiprocessing as mp\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "from tueplots import bundles\n",
        "bundles.icml2024()\n",
        "\n",
        "from torchmetrics import AUROC\n",
        "auroc = AUROC(task=\"binary\")\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "device = \"cuda:0\"\n",
        "\n",
        "def visualize_response_matrix(results, value, filename):\n",
        "    # Extract the groups labels in the order of the columns\n",
        "    group_values = results.columns.get_level_values(\"scenario\")\n",
        "\n",
        "    # Identify the boundaries where the group changes\n",
        "    boundaries = []\n",
        "    for i in range(1, len(group_values)):\n",
        "        if group_values[i] != group_values[i - 1]:\n",
        "            boundaries.append(i - 0.5)  # using 0.5 to place the line between columns\n",
        "\n",
        "    # Visualize the results with a matrix: red is 0, white is -1 and blue is 1\n",
        "    cmap = mcolors.ListedColormap([\"white\", \"red\", \"blue\"])\n",
        "    bounds = [-1.5, -0.5, 0.5, 1.5]\n",
        "    norm = mcolors.BoundaryNorm(bounds, cmap.N)\n",
        "\n",
        "    # Calculate midpoints for each group label\n",
        "    groups_list = list(group_values)\n",
        "    group_names = []\n",
        "    group_midpoints = []\n",
        "    current_group = groups_list[0]\n",
        "    start_index = 0\n",
        "    for i, grp in enumerate(groups_list):\n",
        "        if grp != current_group:\n",
        "            midpoint = (start_index + i - 1) / 2.0\n",
        "            group_names.append(current_group)\n",
        "            group_midpoints.append(midpoint)\n",
        "            current_group = grp\n",
        "            start_index = i\n",
        "    # Add the last group\n",
        "    midpoint = (start_index + len(groups_list) - 1) / 2.0\n",
        "    group_names.append(current_group)\n",
        "    group_midpoints.append(midpoint)\n",
        "\n",
        "    # Define the minimum spacing between labels (e.g., 100 units)\n",
        "    min_spacing = 100\n",
        "    last_label_pos = -float(\"inf\")\n",
        "    # Plot the matrix\n",
        "    with plt.rc_context(bundles.icml2024(usetex=True, family=\"serif\")):\n",
        "        fig, ax = plt.subplots(figsize=(20, 10))\n",
        "        cax = ax.matshow(value, aspect=\"auto\", cmap=cmap, norm=norm)\n",
        "\n",
        "        # Add vertical lines at each boundary\n",
        "        for b in boundaries:\n",
        "            ax.axvline(x=b, color=\"black\", linewidth=0.25, linestyle=\"--\", alpha=0.5)\n",
        "        \n",
        "        # Add group labels above the matrix, only if they're spaced enough apart\n",
        "        for name, pos in zip(group_names, group_midpoints):\n",
        "            if pos - last_label_pos >= min_spacing:\n",
        "                ax.text(pos, -5, name, ha='center', va='bottom', rotation=90, fontsize=3)\n",
        "                last_label_pos = pos\n",
        "\n",
        "        # Add model labels on the y-axis\n",
        "        ax.set_yticks(range(len(results.index)))\n",
        "        ax.set_yticklabels(results.index, fontsize=3)\n",
        "\n",
        "        # Add a colorbar\n",
        "        cbar = plt.colorbar(cax)\n",
        "        cbar.set_ticks([-1, 0, 1])\n",
        "        cbar.set_ticklabels([\"-1\", \"0\", \"1\"])\n",
        "        plt.savefig(filename, dpi=600, bbox_inches=\"tight\")\n",
        "        plt.close()\n",
        "\n",
        "def trainer(parameters, optim, closure, n_iter=100, verbose=True):\n",
        "    pbar = tqdm(range(n_iter)) if verbose else range(n_iter)\n",
        "    for iteration in pbar:\n",
        "        if iteration > 0:\n",
        "            previous_parameters = [p.clone() for p in parameters]\n",
        "            previous_loss = loss.clone()\n",
        "        \n",
        "        loss = optim.step(closure)\n",
        "        \n",
        "        if iteration > 0:\n",
        "            d_loss = (previous_loss - loss).item()\n",
        "            d_parameters = sum(\n",
        "                torch.norm(prev - curr, p=2).item()\n",
        "                for prev, curr in zip(previous_parameters, parameters)\n",
        "            )\n",
        "            grad_norm = sum(torch.norm(p.grad, p=2).item() for p in parameters if p.grad is not None)\n",
        "            if verbose:\n",
        "                pbar.set_postfix({\"grad_norm\": grad_norm, \"d_parameter\": d_parameters, \"d_loss\": d_loss})\n",
        "            \n",
        "            if d_loss < 1e-5 and d_parameters < 1e-5 and grad_norm < 1e-5:\n",
        "                break\n",
        "    return parameters\n",
        "\n",
        "def compute_auc(probs, data, train_idtor, test_idtor):\n",
        "    train_probs = probs[train_idtor.bool()]\n",
        "    test_probs = probs[test_idtor.bool()]\n",
        "    train_labels = data[train_idtor.bool()]\n",
        "    test_labels = data[test_idtor.bool()]\n",
        "    \n",
        "    train_auc = auroc(train_probs, train_labels)\n",
        "    test_auc = auroc(test_probs, test_labels)\n",
        "    print(f\"train auc: {train_auc}\")\n",
        "    print(f\"test auc: {test_auc}\")\n",
        "    \n",
        "    return train_auc, test_auc\n",
        "\n",
        "def compute_cttcorr(probs, data, train_idtor, test_idtor):\n",
        "    train_probs  = probs.clone()\n",
        "    test_probs   = probs.clone()\n",
        "    train_labels = data.clone()\n",
        "    test_labels  = data.clone()\n",
        "\n",
        "    train_mask = ~train_idtor.bool()\n",
        "    train_probs[train_mask]  = float('nan')\n",
        "    train_labels[train_mask] = float('nan')\n",
        "\n",
        "    test_mask = ~test_idtor.bool()\n",
        "    test_probs[test_mask]   = float('nan')\n",
        "    test_labels[test_mask]  = float('nan')\n",
        "    \n",
        "    train_prob_ctt = torch.nanmean(train_probs, dim=1).detach().cpu().numpy()\n",
        "    train_label_ctt = torch.nanmean(train_labels, dim=1).detach().cpu().numpy()\n",
        "    train_mask = ~np.isnan(train_prob_ctt) & ~np.isnan(train_label_ctt)\n",
        "    train_cttcorr = pearsonr(train_prob_ctt[train_mask], train_label_ctt[train_mask]).statistic\n",
        "    \n",
        "    test_prob_ctt = torch.nanmean(test_probs, dim=1).detach().cpu().numpy()\n",
        "    test_label_ctt = torch.nanmean(test_labels, dim=1).detach().cpu().numpy()\n",
        "    test_mask = ~np.isnan(test_prob_ctt) & ~np.isnan(test_label_ctt)\n",
        "    test_cttcorr = pearsonr(test_prob_ctt[test_mask], test_label_ctt[test_mask]).statistic\n",
        "    \n",
        "    print(f\"train cttcorr: {train_cttcorr}\")\n",
        "    print(f\"test cttcorr: {test_cttcorr}\")\n",
        "\n",
        "    return train_cttcorr, test_cttcorr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Override trainer with JSON logging after original definitions\n",
        "import os\n",
        "import json\n",
        "from datetime import datetime\n",
        "from typing import List, Optional\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def _summarize_tensor(t: torch.Tensor):\n",
        "    t_cpu = t.detach().float().cpu()\n",
        "    flat = t_cpu.view(-1)\n",
        "    return {\n",
        "        \"shape\": list(t_cpu.shape),\n",
        "        \"numel\": int(flat.numel()),\n",
        "        \"mean\": float(flat.mean().item()) if flat.numel() > 0 else 0.0,\n",
        "        \"std\": float(flat.std(unbiased=False).item()) if flat.numel() > 1 else 0.0,\n",
        "        \"min\": float(flat.min().item()) if flat.numel() > 0 else 0.0,\n",
        "        \"max\": float(flat.max().item()) if flat.numel() > 0 else 0.0,\n",
        "        \"l2norm\": float(torch.norm(flat, p=2).item()),\n",
        "    }\n",
        "\n",
        "\n",
        "def trainer(\n",
        "    parameters: List[torch.Tensor],\n",
        "    optim,\n",
        "    closure,\n",
        "    n_iter: int = 100,\n",
        "    verbose: bool = True,\n",
        "    log_json_path: Optional[str] = None,\n",
        "    parameter_names: Optional[List[str]] = None,\n",
        "    log_every: int = 1,\n",
        "    save_full_parameters: bool = True,\n",
        "):\n",
        "    if parameter_names is None:\n",
        "        parameter_names = [f\"param_{i}\" for i in range(len(parameters))]\n",
        "\n",
        "    if log_json_path is None:\n",
        "        logs_dir = os.environ.get(\"TRAINER_LOG_DIR\", os.path.join(os.getcwd(), \"logs\"))\n",
        "        os.makedirs(logs_dir, exist_ok=True)\n",
        "        label = \"_\".join(parameter_names) if parameter_names else \"params\"\n",
        "        log_json_path = os.path.join(\n",
        "            logs_dir,\n",
        "            f\"{datetime.utcnow().strftime('%Y%m%dT%H%M%SZ')}_trainer_{label}.jsonl\",\n",
        "        )\n",
        "    else:\n",
        "        os.makedirs(os.path.dirname(log_json_path) or \".\", exist_ok=True)\n",
        "\n",
        "    pbar = tqdm(range(n_iter)) if verbose else range(n_iter)\n",
        "    previous_parameters = None\n",
        "    previous_loss = None\n",
        "\n",
        "    for iteration in pbar:\n",
        "        if iteration > 0:\n",
        "            previous_parameters = [p.clone() for p in parameters]\n",
        "            previous_loss = loss.clone() if 'loss' in locals() else None\n",
        "\n",
        "        loss = optim.step(closure)\n",
        "\n",
        "        if iteration > 0 and previous_parameters is not None and previous_loss is not None:\n",
        "            d_loss = (previous_loss - loss).item()\n",
        "            d_parameters = sum(\n",
        "                torch.norm(prev - curr, p=2).item()\n",
        "                for prev, curr in zip(previous_parameters, parameters)\n",
        "            )\n",
        "        else:\n",
        "            d_loss = None\n",
        "            d_parameters = None\n",
        "\n",
        "        grad_norm = sum(\n",
        "            torch.norm(p.grad, p=2).item() for p in parameters if getattr(p, \"grad\", None) is not None\n",
        "        )\n",
        "\n",
        "        if (iteration % log_every) == 0:\n",
        "            record = {\n",
        "                \"iteration\": int(iteration),\n",
        "                \"loss\": float(loss.item()),\n",
        "                \"grad_norm\": float(grad_norm),\n",
        "            }\n",
        "            if d_loss is not None:\n",
        "                record[\"d_loss\"] = float(d_loss)\n",
        "            if d_parameters is not None:\n",
        "                record[\"d_parameters\"] = float(d_parameters)\n",
        "\n",
        "            if save_full_parameters:\n",
        "                record[\"parameters\"] = {\n",
        "                    name: p.detach().float().cpu().tolist() for name, p in zip(parameter_names, parameters)\n",
        "                }\n",
        "            else:\n",
        "                record[\"parameter_summary\"] = {\n",
        "                    name: _summarize_tensor(p) for name, p in zip(parameter_names, parameters)\n",
        "                }\n",
        "\n",
        "            with open(log_json_path, \"a\") as fp:\n",
        "                fp.write(json.dumps(record) + \"\\n\")\n",
        "\n",
        "        if verbose and isinstance(pbar, tqdm):\n",
        "            pbar.set_postfix(\n",
        "                {\n",
        "                    \"grad_norm\": grad_norm,\n",
        "                    \"d_param\": d_parameters if d_parameters is not None else 0.0,\n",
        "                    \"d_loss\": d_loss if d_loss is not None else 0.0,\n",
        "                    \"loss\": float(loss.item()),\n",
        "                }\n",
        "            )\n",
        "\n",
        "        if (\n",
        "            d_loss is not None\n",
        "            and d_parameters is not None\n",
        "            and abs(d_loss) < 1e-5\n",
        "            and d_parameters < 1e-5\n",
        "            and grad_norm < 1e-5\n",
        "        ):\n",
        "            break\n",
        "\n",
        "    return parameters\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(f\"../data/resmat.pkl\", \"rb\") as f:\n",
        "    results = pickle.load(f)\n",
        "    \n",
        "# data_withnan, missing=nan\n",
        "# data_withneg1, missing=-1\n",
        "# data_with0, missing=0\n",
        "data_withnan = torch.tensor(results.values, dtype=torch.float, device=device)\n",
        "data_withneg1 = data_withnan.nan_to_num(nan=-1.0)\n",
        "data_idtor = (data_withneg1 != -1).to(float)\n",
        "data_with0 = data_withneg1 * data_idtor # -1 -> 0\n",
        "n_test_takers, n_items = data_with0.shape\n",
        "scenarios = results.columns.get_level_values(\"scenario\").unique()\n",
        "\n",
        "# save dict\n",
        "metric_results = defaultdict(dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of test takers: 183\n",
            "Number of items: 78712\n",
            "Number of scenarios: 22\n",
            "air_bench_2024: 41 test takers, 4985 items\n",
            "babi_qa: 70 test takers, 3461 items\n",
            "bbq: 42 test takers, 999 items\n",
            "boolq: 67 test takers, 3316 items\n",
            "civil_comments: 67 test takers, 29407 items\n",
            "commonsense: 91 test takers, 498 items\n",
            "dyck_language_np=3: 69 test takers, 500 items\n",
            "entity_data_imputation: 67 test takers, 395 items\n",
            "entity_matching: 67 test takers, 1396 items\n",
            "gsm: 90 test takers, 997 items\n",
            "imdb: 67 test takers, 3530 items\n",
            "legal_support: 69 test takers, 594 items\n",
            "legalbench: 91 test takers, 1997 items\n",
            "lsat_qa: 69 test takers, 454 items\n",
            "math: 91 test takers, 436 items\n",
            "med_qa: 91 test takers, 998 items\n",
            "mmlu: 79 test takers, 13223 items\n",
            "raft: 67 test takers, 1336 items\n",
            "synthetic_reasoning: 69 test takers, 2234 items\n",
            "thai_exam: 40 test takers, 557 items\n",
            "truthful_qa: 67 test takers, 1888 items\n",
            "wikifact: 67 test takers, 5511 items\n"
          ]
        }
      ],
      "source": [
        "vis_resmat_dir = \"../result/visualize_resmat\"\n",
        "os.makedirs(vis_resmat_dir, exist_ok=True)\n",
        "\n",
        "# overall stats\n",
        "print(\"Number of test takers:\", results.shape[0])\n",
        "print(\"Number of items:\", results.shape[1])\n",
        "print(\"Number of scenarios:\", results.columns.get_level_values(\"scenario\").nunique())\n",
        "visualize_response_matrix(results, results, f\"{vis_resmat_dir}/resmat_all\")\n",
        "\n",
        "# count the number of items and test takers in each dataset\n",
        "scenario_counts = {}\n",
        "for scenario in sorted(scenarios):\n",
        "    mask = results.columns.get_level_values(\"scenario\") == scenario\n",
        "    sub_results = results.loc[:, mask]\n",
        "    scenario_counts[scenario] = {\n",
        "        \"n_items\": sub_results.shape[1],\n",
        "        \"n_test_takers\": sub_results.notna().any(axis=1).sum()\n",
        "    }\n",
        "    print(f\"{scenario}: {scenario_counts[scenario]['n_test_takers']} test takers, {scenario_counts[scenario]['n_items']} items\")\n",
        "    # visualize_response_matrix(sub_results, sub_results, f\"{vis_resmat_dir}/resmat_{scenario}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Naive prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Naive auc 0: 0.5\n",
            "Naive auc 1.1: 0.8070003986358643\n",
            "Naive auc 1.2: 0.6573556065559387\n"
          ]
        }
      ],
      "source": [
        "# overall mean\n",
        "naive_prediction_0 = torch.nanmean(data_withnan)\n",
        "naive_prediction_0 = naive_prediction_0.expand(data_withnan.shape[0], data_withnan.shape[1])\n",
        "auc_train_0 = auroc(naive_prediction_0[data_idtor.bool()], data_withnan[data_idtor.bool()]).item()\n",
        "print(f\"Naive auc 0: {auc_train_0}\")\n",
        "\n",
        "# question difficulty average\n",
        "naive_prediction_1 = torch.nanmean(data_withnan, dim=0)\n",
        "naive_prediction_1 = naive_prediction_1[None, :].expand(data_withnan.shape[0], data_withnan.shape[1])\n",
        "auc_train_1_1 = auroc(naive_prediction_1[data_idtor.bool()], data_withnan[data_idtor.bool()]).item()\n",
        "print(f\"Naive auc 1.1: {auc_train_1_1}\")\n",
        "\n",
        "# test taker average\n",
        "naive_prediction_1 = torch.nanmean(data_withnan, dim=1)\n",
        "naive_prediction_1 = naive_prediction_1[:, None].expand(data_withnan.shape[0], data_withnan.shape[1])\n",
        "auc_train_1_2 = auroc(naive_prediction_1[data_idtor.bool()], data_withnan[data_idtor.bool()]).item()\n",
        "print(f\"Naive auc 1.2: {auc_train_1_2}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Simple Rasch model\n",
        "\n",
        "fit one theta for all datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trial 0 valid condition: True\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | 0/2 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  3%|███████████████████████████▌                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | 3/100 [00:15<08:19,  5.15s/it, grad_norm=2.68e-7, d_parameter=0, d_loss=0]\n",
            "  3%|███████████████████████████▌                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | 3/100 [00:09<05:10,  3.20s/it, grad_norm=1.08e-6, d_parameter=0, d_loss=0]\n",
            "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:25<00:00, 12.76s/it]\n",
            " 13%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | 13/100 [00:00<00:04, 19.32it/s, grad_norm=2.21e-7, d_parameter=0, d_loss=8.76e-11]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train auc: 0.8402137756347656\n",
            "test auc: 0.8265656232833862\n",
            "train cttcorr: 0.999998942784063\n",
            "test cttcorr: 0.9927187362960916\n"
          ]
        }
      ],
      "source": [
        "valid_condition = False\n",
        "trial = 0 \n",
        "# If a test-taker or an item has no data in the training set, the model has no information to learn their ability\n",
        "# or difficulty. The while loop keeps trying new random splits until this crucial condition is met,\n",
        "# preventing errors during training.\n",
        "# Valid split means every person has at least 1 train item AND every item has at least 1 train response\n",
        "while not valid_condition:\n",
        "    # Create random binary mask: 80% chance each observed data point goes to training\n",
        "    # This is the authors' train-test splitting method\n",
        "    train_idtor = torch.bernoulli(data_idtor * 0.8).int()\n",
        "    test_idtor = data_idtor - train_idtor\n",
        "    valid_condition = (train_idtor.sum(axis=1) != 0).all() and (train_idtor.sum(axis=0) != 0).all()\n",
        "    print(f\"trial {trial} valid condition: {valid_condition}\")\n",
        "    trial += 1\n",
        "\n",
        "# Fit item difficulties (z parameters) in batch\n",
        "B = 50000\n",
        "optimized_zs = []\n",
        "# Initialising 150 thetas of dimension of n_test_takers with random values\n",
        "# This is to perform Monte Carlo simulation for unknown random theta\n",
        "thetas_nuisance = torch.randn(150, n_test_takers, device=device)\n",
        "\n",
        "# For each batched items, we fix thetas with random variables then optimise z for each question following LBFGS\n",
        "for i in tqdm(range(0, n_items, B)):    # Loop through items in chunks of size B\n",
        "    data_batch = data_with0[:, i:i+B]   # Get current batch of item responses\n",
        "    train_idtor_batch = train_idtor[:, i:i+B]   # Get training mask for current batch\n",
        "    current_B = data_batch.shape[1] # Actual batch size (last batch might be smaller)\n",
        "    \n",
        "    # Initialize item difficulties for this batch randomly\n",
        "    z_i = torch.randn(current_B, requires_grad=True, device=device)\n",
        "    # Set up L-BFGS optimizer for this batch of item difficulties\n",
        "    optim_z_i = LBFGS([z_i], lr=0.1, max_iter=20, history_size=10, line_search_fn=\"strong_wolfe\")\n",
        "    \n",
        "    def closure_z_i():  # Closure function required by L-BFGS\n",
        "        optim_z_i.zero_grad()\n",
        "        probs = torch.sigmoid(thetas_nuisance[:, :, None] + z_i[None, None, :])\n",
        "        # Compute negative log-likelihood loss (only on training data)\n",
        "        # train_idtor is 0 for test items, this is to mask away the test items and keep only training data\n",
        "        # for log_prob, each data entry that is incorrect is 0, and is calculated as log(1-p). Whereas, correct ones are log(p)\n",
        "        # p is the bernoulli probability with the given distribution (for each item, and each dimension)\n",
        "        # averaging out the loss over 150 samples to finish Monte-Carlo simulation\n",
        "        loss = -(Bernoulli(probs=probs).log_prob(data_batch)*train_idtor_batch).mean()\n",
        "        # Compute gradients, by back-propagating all the previous elements (each item in each dimension)\n",
        "        # inside the tensor (zs, this happens automatically cuz PyTorch is GOAT)\n",
        "        loss.backward()\n",
        "        return loss\n",
        "    z_i_optimized = trainer([z_i], optim_z_i, closure_z_i, parameter_names=[\"z\"], log_json_path=os.path.join(\"../result/logs\", f\"rasch_global_z_batch_{i}_{i+current_B}.jsonl\"))[0].detach()\n",
        "    optimized_zs.append(z_i_optimized)  # Store optimized difficulties\n",
        "\n",
        "zs = torch.cat(optimized_zs)\n",
        "\n",
        "# Fit person abilities (theta parameters) using all items with fixed difficulties\n",
        "thetas = torch.randn(n_test_takers, requires_grad=True, device=device)  # Initialize abilities randomly\n",
        "# Set up L-BFGS optimizer for person abilities\n",
        "optim_theta = LBFGS([thetas], lr=0.1, max_iter=20, history_size=10, line_search_fn=\"strong_wolfe\")\n",
        "\n",
        "def closure_theta():  # Closure function for optimizing abilities same as difficulties (closure_z_i)\n",
        "    optim_theta.zero_grad()  # Clear gradients\n",
        "    # Compute probabilities using estimated abilities and fixed item difficulties\n",
        "    probs = torch.sigmoid(thetas[:, None] + zs[None, :])\n",
        "    loss = -(Bernoulli(probs=probs).log_prob(data_with0)*train_idtor).mean()\n",
        "    loss.backward()\n",
        "    return loss\n",
        "\n",
        "thetas = trainer([thetas], optim_theta, closure_theta, parameter_names=[\"theta\"], log_json_path=os.path.join(\"../result/logs\", \"rasch_global_theta.jsonl\"))[0]\n",
        "\n",
        "# Calculate final model predictions using optimized parameters\n",
        "probs = torch.sigmoid(thetas[:, None] + zs[None, :])  # P(correct) = sigmoid(ability + difficulty)\n",
        "\n",
        "# Evaluate model performance using AUC (Area Under ROC Curve)\n",
        "train_auc, test_auc = compute_auc(probs, data_with0, train_idtor, test_idtor)\n",
        "metric_results[\"combined_data\"][\"train_auc\"] = train_auc.item()  # Store training AUC\n",
        "metric_results[\"combined_data\"][\"test_auc\"] = test_auc.item()   # Store test AUC\n",
        "\n",
        "# Evaluate model performance using CTT correlation (Classical Test Theory)\n",
        "train_cttcorr, test_cttcorr = compute_cttcorr(probs, data_with0, train_idtor, test_idtor)\n",
        "metric_results[\"combined_data\"][\"train_cttcorr\"] = train_cttcorr.item()  # Store training correlation\n",
        "metric_results[\"combined_data\"][\"test_cttcorr\"] = test_cttcorr.item()   # Store test correlation\n",
        "\n",
        "# Clean up memory by deleting large objects and clearing GPU cache\n",
        "del optim_theta, thetas, z_i, thetas_nuisance, optim_z_i\n",
        "gc.collect()  # Force garbage collection\n",
        "torch.cuda.empty_cache()  # Clear GPU memory cache"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Rasch\n",
        "\n",
        "fit one theta for each dataset\n",
        "\n",
        "reuses z from before"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "lsat_qa\n",
            "train auc: 0.6885020732879639\n",
            "test auc: 0.6195417642593384\n",
            "train cttcorr: 0.9999999972999117\n",
            "test cttcorr: 0.5914744119124559\n",
            "\n",
            "truthful_qa\n",
            "train auc: 0.7754521369934082\n",
            "test auc: 0.7521977424621582\n",
            "train cttcorr: 0.9999999890975424\n",
            "test cttcorr: 0.9705701018338492\n",
            "\n",
            "synthetic_reasoning\n",
            "train auc: 0.8787851929664612\n",
            "test auc: 0.8684341311454773\n",
            "train cttcorr: 0.9999999998367093\n",
            "test cttcorr: 0.9944160113974599\n",
            "\n",
            "babi_qa\n",
            "train auc: 0.8385903239250183\n",
            "test auc: 0.82568359375\n",
            "train cttcorr: 0.9999999978926333\n",
            "test cttcorr: 0.9801552224953756\n",
            "\n",
            "wikifact\n",
            "train auc: 0.8949174880981445\n",
            "test auc: 0.8831348419189453\n",
            "train cttcorr: 0.999999998540401\n",
            "test cttcorr: 0.9955537306021037\n",
            "\n",
            "bbq\n",
            "train auc: 0.7258046865463257\n",
            "test auc: 0.6775155067443848\n",
            "train cttcorr: 0.9999999966710638\n",
            "test cttcorr: 0.9851155507875281\n",
            "\n",
            "thai_exam\n",
            "train auc: 0.857990562915802\n",
            "test auc: 0.8283470273017883\n",
            "train cttcorr: 0.99999999333132\n",
            "test cttcorr: 0.9472261461296729\n",
            "\n",
            "dyck_language_np=3\n",
            "train auc: 0.7988154888153076\n",
            "test auc: 0.7704381942749023\n",
            "train cttcorr: 0.9999999992960812\n",
            "test cttcorr: 0.9523210492777567\n",
            "\n",
            "legal_support\n",
            "train auc: 0.7143197059631348\n",
            "test auc: 0.6697124242782593\n",
            "train cttcorr: 0.9999999980225399\n",
            "test cttcorr: 0.8647853588757385\n",
            "\n",
            "civil_comments\n",
            "train auc: 0.7923047542572021\n",
            "test auc: 0.7745755314826965\n",
            "train cttcorr: 0.9999999980694246\n",
            "test cttcorr: 0.9993406257099393\n",
            "\n",
            "legalbench\n",
            "train auc: 0.8490581512451172\n",
            "test auc: 0.8361703753471375\n",
            "train cttcorr: 0.9999999910057704\n",
            "test cttcorr: 0.9853104531988046\n",
            "\n",
            "raft\n",
            "train auc: 0.8433385491371155\n",
            "test auc: 0.8358889818191528\n",
            "train cttcorr: 0.9999999911087012\n",
            "test cttcorr: 0.9668131375288304\n",
            "\n",
            "air_bench_2024\n",
            "train auc: 0.9190464615821838\n",
            "test auc: 0.9031293392181396\n",
            "train cttcorr: 0.9999999966552328\n",
            "test cttcorr: 0.9979794422284815\n",
            "\n",
            "math\n",
            "train auc: 0.9069644808769226\n",
            "test auc: 0.8990507125854492\n",
            "train cttcorr: 0.9999999993639423\n",
            "test cttcorr: 0.9904061841428299\n",
            "\n",
            "med_qa\n",
            "train auc: 0.8781101703643799\n",
            "test auc: 0.8690382838249207\n",
            "train cttcorr: 0.9999999980198907\n",
            "test cttcorr: 0.9823301510143873\n",
            "\n",
            "gsm\n",
            "train auc: 0.9063451290130615\n",
            "test auc: 0.8985934257507324\n",
            "train cttcorr: 0.9999999995077986\n",
            "test cttcorr: 0.9927646876819026\n",
            "\n",
            "boolq\n",
            "train auc: 0.8516708612442017\n",
            "test auc: 0.8336220383644104\n",
            "train cttcorr: 0.9999999996805106\n",
            "test cttcorr: 0.9930852318276311\n",
            "\n",
            "mmlu\n",
            "train auc: 0.9107842445373535\n",
            "test auc: 0.8996495008468628\n",
            "train cttcorr: 0.9999999965805638\n",
            "test cttcorr: 0.9972730818682869\n",
            "\n",
            "entity_matching\n",
            "train auc: 0.900503396987915\n",
            "test auc: 0.8869065046310425\n",
            "train cttcorr: 0.9999999997283641\n",
            "test cttcorr: 0.9967753770435961\n",
            "\n",
            "entity_data_imputation\n",
            "train auc: 0.9435975551605225\n",
            "test auc: 0.9344534277915955\n",
            "train cttcorr: 0.9999999996517601\n",
            "test cttcorr: 0.9643957319227096\n",
            "\n",
            "commonsense\n",
            "train auc: 0.9296249747276306\n",
            "test auc: 0.9214940071105957\n",
            "train cttcorr: 0.9999999991810357\n",
            "test cttcorr: 0.9817117544667912\n",
            "\n",
            "imdb\n",
            "train auc: 0.9274917244911194\n",
            "test auc: 0.8960437774658203\n",
            "train cttcorr: 0.9999999889199367\n",
            "test cttcorr: 0.9927331523235726\n"
          ]
        }
      ],
      "source": [
        "# Rasch per-scenario calibration: reuse globally estimated item difficulties (zs)\n",
        "# and re-estimate person abilities (theta) separately for each scenario subset.\n",
        "calres_dir = \"../data/calibration_result\"            # Directory to store calibration outputs\n",
        "os.makedirs(calres_dir, exist_ok=True)                 # Create directory if it doesn't exist (no error if it does)\n",
        "\n",
        "for scenario in scenarios:                             # Iterate over each scenario (dataset / item subset)\n",
        "    print(f\"\\n{scenario}\")\n",
        "    mask = (results.columns.get_level_values(\"scenario\") == scenario)   # Boolean mask selecting this scenario's columns\n",
        "    data_scenario = data_with0[:, mask]                # Responses (persons × items_in_scenario), missing already 0\n",
        "    train_idtor_scenario = train_idtor[:, mask]        # Train indicator restricted to scenario items\n",
        "    test_idtor_scenario  = test_idtor[:, mask]         # Test indicator restricted likewise\n",
        "    z_scenario = zs[mask]                              # Item difficulty parameters for this scenario (taken from global fit)\n",
        "\n",
        "    # Save item difficulties for traceability / later analysis\n",
        "    df_z = pd.DataFrame({\"z\": z_scenario.detach().cpu().numpy()})\n",
        "    df_z.to_csv(f'{calres_dir}/z_{scenario}.csv', index=False)\n",
        "\n",
        "    # Initialize person abilities (theta) at zero (identifiability: Rasch shift absorbed in z already)\n",
        "    thetas = torch.zeros(n_test_takers, requires_grad=True, device=device)\n",
        "    # L-BFGS optimizer (quasi-Newton) usually converges faster for small parameter vectors\n",
        "    optim_theta = torch.optim.LBFGS(\n",
        "        [thetas], lr=0.1, max_iter=20, history_size=10, line_search_fn=\"strong_wolfe\"\n",
        "    )\n",
        "\n",
        "    def closure_theta():                               # Closure required by LBFGS to recompute loss & gradients\n",
        "        optim_theta.zero_grad()                        # Clear previous gradients\n",
        "        probs = torch.sigmoid(thetas[:, None] + z_scenario[None, :])   # Rasch: P_ij = sigmoid(theta_i + z_j)\n",
        "        # Negative log-likelihood over Bernoulli responses, masking to training cells only\n",
        "        # (data_scenario ∈ {0,1}; train_idtor_scenario zeros out non-train cells)\n",
        "        loss = -(Bernoulli(probs=probs).log_prob(data_scenario) * train_idtor_scenario).mean()\n",
        "        loss.backward()                                # Backprop to compute gradients w.r.t thetas\n",
        "        return loss\n",
        "\n",
        "    # Optimize theta parameters (trainer wraps repeated optimizer.step calls + early stopping)\n",
        "    thetas = trainer([thetas], optim_theta, closure_theta, verbose=False, parameter_names=[\"theta\"], log_json_path=os.path.join(\"../result/logs\", f\"rasch_scenario_{scenario}_theta.jsonl\"))[0]\n",
        "\n",
        "    # Materialize and save non-trivial (non-zero) theta estimates.\n",
        "    # Zero values usually correspond to persons with no training responses in this scenario.\n",
        "    df_theta = pd.DataFrame({\"theta\": thetas.detach().cpu().numpy()})\n",
        "    df_theta = df_theta[df_theta[\"theta\"] != 0]       # Filter out untouched thetas (optional heuristic)\n",
        "    df_theta.to_csv(f'{calres_dir}/theta_{scenario}.csv', index=False)\n",
        "\n",
        "    # Recompute probabilities with fitted thetas for evaluation\n",
        "    probs = torch.sigmoid(thetas[:, None] + z_scenario[None, :])\n",
        "\n",
        "    # AUC on train and test partitions\n",
        "    train_auc, test_auc = compute_auc(probs, data_scenario, train_idtor_scenario, test_idtor_scenario)\n",
        "    metric_results[scenario][\"train_auc\"] = train_auc.item()\n",
        "    metric_results[scenario][\"test_auc\"]  = test_auc.item()\n",
        "    \n",
        "    # Classical Test Theory style correlation (mean predicted vs mean observed per person)\n",
        "    train_cttcorr, test_cttcorr = compute_cttcorr(probs, data_scenario, train_idtor_scenario, test_idtor_scenario)\n",
        "    metric_results[scenario][\"train_cttcorr\"] = train_cttcorr.item()\n",
        "    metric_results[scenario][\"test_cttcorr\"]  = test_cttcorr.item()\n",
        "\n",
        "# Free large tensors / optimizer objects to release memory (especially GPU)\n",
        "del zs, optim_theta, thetas\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x250 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "values = [\n",
        "    auc_train_0, auc_train_1_2, auc_train_1_1, \n",
        "    metric_results[\"combined_data\"][\"test_auc\"],\n",
        "    np.max(np.array([metric_results[scenario][\"test_auc\"] for scenario in scenarios]))\n",
        "]\n",
        "# Difficulty modeling is the approximation of zs, the first stage of Simple Rasch\n",
        "labels = ['Naive', 'Average score', 'Difficulty modeling', 'Simple Rasch', 'Rasch']\n",
        "\n",
        "with plt.rc_context(bundles.icml2024(usetex=True, family=\"serif\")):\n",
        "    fig, ax = plt.subplots(figsize=(10, 2.5))\n",
        "    bars = ax.bar(labels, values, color='#b37fbf')\n",
        "\n",
        "    ax.set_xlabel(r'Response Model', fontsize=15)\n",
        "    ax.set_ylabel(r'AUC on Test Set', fontsize=18)\n",
        "    ax.set_ylim(0.4, 1.0)\n",
        "    ax.set_yticks([i/5 for i in range(3, 6)])\n",
        "    plt.tick_params(axis=\"both\", labelsize=12)\n",
        "\n",
        "    for bar, v in zip(bars, values):\n",
        "        ax.text(bar.get_x() + bar.get_width() / 2, v + 0.01, f'{v:.2f}', \n",
        "                ha='center', va='bottom', fontsize=12)\n",
        "    \n",
        "    plt.savefig(\"../result/test_auc_grow.png\", dpi=300, bbox_inches=\"tight\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Amortized Difficulty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "embed_dim: 4096\n"
          ]
        }
      ],
      "source": [
        "# Check file gather_helm_data/embed.py for the embedding generation\n",
        "with open(f\"../data/embed_meta-llama_Llama-3.1-8B-Instruct.pkl\", \"rb\") as f:\n",
        "    df_embed = pickle.load(f)\n",
        "question_to_emb = dict(zip(df_embed[\"question\"], df_embed[\"embedding\"]))\n",
        "questions = results.columns.get_level_values(\"input.text\").tolist()\n",
        "embeds = [question_to_emb.get(q, None) for q in questions]\n",
        "embed_dim = len(next(item for item in embeds if item is not None))\n",
        "print(f\"embed_dim: {embed_dim}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "lsat_qa\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 63%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                                                                                                                                                                                                                                                | 63/100 [00:05<00:03, 11.20it/s, grad_norm=1.55e-6, d_parameter=0, d_loss=2.17e-10]\n",
            "  7%|███████████████████████████████████████████████████████████████▊                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | 7/100 [00:00<00:02, 34.54it/s, grad_norm=2.37e-7, d_parameter=0, d_loss=4.16e-9]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train auc: 0.6763260364532471\n",
            "test auc: 0.533271074295044\n",
            "train cttcorr: 0.9999999855571661\n",
            "test cttcorr: 0.6666514225457503\n",
            "\n",
            "truthful_qa\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:34<00:00,  2.86it/s, grad_norm=6.51e-6, d_parameter=0.36, d_loss=7.37e-8]\n",
            " 12%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | 12/100 [00:00<00:00, 107.07it/s, grad_norm=5.75e-7, d_parameter=0, d_loss=1.65e-10]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train auc: 0.7722413539886475\n",
            "test auc: 0.7296116948127747\n",
            "train cttcorr: 0.9999999899322035\n",
            "test cttcorr: 0.9830719203367174\n",
            "\n",
            "synthetic_reasoning\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:41<00:00,  2.43it/s, grad_norm=2.54e-5, d_parameter=3.56, d_loss=3.09e-6]\n",
            "  8%|████████████████████████████████████████████████████████████████████████▉                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | 8/100 [00:00<00:01, 52.06it/s, grad_norm=2.49e-7, d_parameter=0, d_loss=2.08e-9]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train auc: 0.8777711391448975\n",
            "test auc: 0.7192123532295227\n",
            "train cttcorr: 0.9999999991950111\n",
            "test cttcorr: 0.9937904613333027\n",
            "\n",
            "babi_qa\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:00<00:00,  1.65it/s, grad_norm=0.000168, d_parameter=5.03, d_loss=3.92e-5]\n",
            "  6%|██████████████████████████████████████████████████████▋                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | 6/100 [00:00<00:03, 30.88it/s, grad_norm=1.89e-7, d_parameter=0, d_loss=3.59e-9]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train auc: 0.8309568166732788\n",
            "test auc: 0.7370664477348328\n",
            "train cttcorr: 0.9999999981787767\n",
            "test cttcorr: 0.9827419597534225\n",
            "\n",
            "wikifact\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:34<00:00,  1.06it/s, grad_norm=9.81e-5, d_parameter=7.86, d_loss=2.21e-5]\n",
            " 11%|████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | 11/100 [00:00<00:00, 90.50it/s, grad_norm=2.17e-7, d_parameter=0, d_loss=1.03e-10]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train auc: 0.8895800113677979\n",
            "test auc: 0.653024435043335\n",
            "train cttcorr: 0.999999998868991\n",
            "test cttcorr: 0.9921132490586263\n",
            "\n",
            "bbq\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 88%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                             | 88/100 [00:15<00:02,  5.53it/s, grad_norm=1.4e-6, d_parameter=0, d_loss=2.52e-8]\n",
            " 12%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | 12/100 [00:00<00:01, 80.35it/s, grad_norm=5.58e-7, d_parameter=0, d_loss=-1.53e-11]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train auc: 0.7185817956924438\n",
            "test auc: 0.6290475130081177\n",
            "train cttcorr: 0.9999999936946713\n",
            "test cttcorr: 0.9826839304952918\n",
            "\n",
            "thai_exam\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 81%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                             | 81/100 [00:08<00:02,  9.46it/s, grad_norm=2.11e-6, d_parameter=0, d_loss=2.08e-10]\n",
            "  2%|██████████████████▎                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | 2/100 [00:00<00:02, 34.71it/s, grad_norm=6.3e-7, d_parameter=0, d_loss=1.95e-9]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train auc: 0.8551082015037537\n",
            "test auc: 0.6553918719291687\n",
            "train cttcorr: 0.9999999909008068\n",
            "test cttcorr: 0.9563553001283531\n",
            "\n",
            "dyck_language_np=3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:09<00:00, 10.76it/s, grad_norm=8.39e-6, d_parameter=0.0143, d_loss=8.37e-9]\n",
            " 12%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | 12/100 [00:00<00:01, 59.19it/s, grad_norm=2.45e-7, d_parameter=0, d_loss=9.83e-11]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train auc: 0.7979531288146973\n",
            "test auc: 0.7311034202575684\n",
            "train cttcorr: 0.9999999994393965\n",
            "test cttcorr: 0.9252554053045023\n",
            "\n",
            "legal_support\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 51%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                                                                                                                                                                                                                                                                                                              | 51/100 [00:04<00:04, 10.83it/s, grad_norm=1.62e-6, d_parameter=0, d_loss=4.78e-10]\n",
            " 10%|███████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | 10/100 [00:00<00:02, 39.25it/s, grad_norm=2.49e-7, d_parameter=0, d_loss=2.12e-9]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train auc: 0.7111147046089172\n",
            "test auc: 0.5263113975524902\n",
            "train cttcorr: 0.9999999978016798\n",
            "test cttcorr: 0.8984408634834505\n",
            "\n",
            "civil_comments\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [08:31<00:00,  5.12s/it, grad_norm=0.000198, d_parameter=1.43, d_loss=6.09e-6]\n",
            " 12%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | 12/100 [00:00<00:01, 54.50it/s, grad_norm=4.36e-7, d_parameter=0, d_loss=9.96e-11]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train auc: 0.759771466255188\n",
            "test auc: 0.7547475099563599\n",
            "train cttcorr: 0.9999999980620158\n",
            "test cttcorr: 0.9996480558965548\n",
            "\n",
            "legalbench\n",
            "\n",
            "legalbench\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:37<00:00,  2.69it/s, grad_norm=9.53e-5, d_parameter=7.41, d_loss=2.22e-5]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:37<00:00,  2.69it/s, grad_norm=9.53e-5, d_parameter=7.41, d_loss=2.22e-5]\n",
            "  4%|████████████████████████████████████▍                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | 4/100 [00:00<00:01, 59.04it/s, grad_norm=3.91e-7, d_parameter=0, d_loss=6.47e-10]\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train auc: 0.84792560338974\n",
            "test auc: 0.6275990009307861\n",
            "train cttcorr: 0.9999999946757762\n",
            "test cttcorr: 0.9735571779462975\n",
            "\n",
            "raft\n",
            "\n",
            "raft\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:26<00:00,  3.76it/s, grad_norm=3.04e-6, d_parameter=0.614, d_loss=9.39e-8]\n",
            "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:26<00:00,  3.76it/s, grad_norm=3.04e-6, d_parameter=0.614, d_loss=9.39e-8]\n",
            "  7%|███████████████████████████████████████████████████████████████▋                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | 7/100 [00:00<00:01, 82.47it/s, grad_norm=6.92e-7, d_parameter=0, d_loss=-1.01e-11]\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train auc: 0.8404725790023804\n",
            "test auc: 0.8156154751777649\n",
            "train cttcorr: 0.9999999883784033\n",
            "test cttcorr: 0.9724920697462726\n",
            "\n",
            "air_bench_2024\n",
            "\n",
            "air_bench_2024\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:25<00:00,  1.17it/s, grad_norm=4.62e-5, d_parameter=8.47, d_loss=1.25e-5]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:25<00:00,  1.17it/s, grad_norm=4.62e-5, d_parameter=8.47, d_loss=1.25e-5]\n",
            "  3%|███████████████████████████▎                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | 3/100 [00:00<00:01, 56.56it/s, grad_norm=4.03e-7, d_parameter=0, d_loss=6.3e-10]\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train auc: 0.9156495928764343\n",
            "test auc: 0.737615704536438\n",
            "train cttcorr: 0.9999999972975504\n",
            "test cttcorr: 0.9957386983790383\n",
            "\n",
            "math\n",
            "\n",
            "math\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 55%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                                                                                                                                                                                                                                                                          | 55/100 [00:05<00:04, 10.91it/s, grad_norm=1.2e-6, d_parameter=0, d_loss=2.52e-9]\n",
            " 55%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                                                                                                                                                                                                                                                                          | 55/100 [00:05<00:04, 10.91it/s, grad_norm=1.2e-6, d_parameter=0, d_loss=2.52e-9]\n",
            " 10%|██████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | 10/100 [00:00<00:01, 73.72it/s, grad_norm=4.24e-7, d_parameter=0, d_loss=-1.22e-10]\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train auc: 0.906830906867981\n",
            "test auc: 0.8398342728614807\n",
            "train cttcorr: 0.999999999398002\n",
            "test cttcorr: 0.9884423849490999\n",
            "\n",
            "med_qa\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 65%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                                                                                                                                                                            | 65/100 [00:12<00:04,  8.17it/s, grad_norm=5.62e-6, d_parameter=0.00967, d_loss=2.05e-10]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train auc: 0.8430184721946716\n",
            "test auc: 0.6811718940734863\n",
            "train cttcorr: 0.9999999914178419\n",
            "test cttcorr: 0.9978155901873424\n",
            "\n",
            "entity_matching\n",
            "\n",
            "entity_matching\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:27<00:00,  3.62it/s, grad_norm=9.73e-6, d_parameter=1.95, d_loss=6.36e-7]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:27<00:00,  3.62it/s, grad_norm=9.73e-6, d_parameter=1.95, d_loss=6.36e-7]\n",
            "  6%|██████████████████████████████████████████████████████▋                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | 6/100 [00:00<00:02, 34.93it/s, grad_norm=2.03e-7, d_parameter=0, d_loss=1.99e-9]\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train auc: 0.902068018913269\n",
            "test auc: 0.8638792634010315\n",
            "train cttcorr: 0.9999999996876143\n",
            "test cttcorr: 0.9887966801103973\n",
            "\n",
            "entity_data_imputation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:08<00:00, 12.19it/s, grad_norm=9.16e-6, d_parameter=0.00495, d_loss=3.68e-10]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:08<00:00, 12.19it/s, grad_norm=9.16e-6, d_parameter=0.00495, d_loss=3.68e-10]\n",
            "  4%|████████████████████████████████████▍                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | 4/100 [00:00<00:04, 20.49it/s, grad_norm=2.65e-7, d_parameter=0, d_loss=2.93e-7]\n",
            "  4%|████████████████████████████████████▍                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | 4/100 [00:00<00:04, 20.49it/s, grad_norm=2.65e-7, d_parameter=0, d_loss=2.93e-7]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train auc: 0.9440808296203613\n",
            "test auc: 0.8954769372940063\n",
            "train cttcorr: 0.9999999986288406\n",
            "test cttcorr: 0.9671064043247249\n",
            "\n",
            "commonsense\n",
            "\n",
            "commonsense\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 55%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                                                                                                                                                                                                                                                                         | 55/100 [00:03<00:02, 16.88it/s, grad_norm=1.97e-6, d_parameter=0, d_loss=2.08e-10]\n",
            " 55%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                                                                                                                                                                                                                                                                         | 55/100 [00:03<00:02, 16.88it/s, grad_norm=1.97e-6, d_parameter=0, d_loss=2.08e-10]\n",
            " 11%|███████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | 11/100 [00:00<00:01, 77.77it/s, grad_norm=3.69e-7, d_parameter=0, d_loss=-3.36e-10]\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train auc: 0.9306890964508057\n",
            "test auc: 0.7688260078430176\n",
            "train cttcorr: 0.9999999987199036\n",
            "test cttcorr: 0.9838781467063344\n",
            "\n",
            "imdb\n",
            "\n",
            "imdb\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:01<00:00,  1.62it/s, grad_norm=2.15e-5, d_parameter=5.13, d_loss=8.15e-7]\n",
            "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [01:01<00:00,  1.62it/s, grad_norm=2.15e-5, d_parameter=5.13, d_loss=8.15e-7]\n",
            " 10%|███████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | 10/100 [00:00<00:01, 77.60it/s, grad_norm=2.35e-7, d_parameter=0, d_loss=9.92e-11]\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train auc: 0.9256952404975891\n",
            "test auc: 0.8482758402824402\n",
            "train cttcorr: 0.9999999878192919\n",
            "test cttcorr: 0.9919104178338615\n",
            "\n",
            "combined_data\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [07:31<00:00,  4.52s/it, grad_norm=0.00174, d_parameter=0.0444, d_loss=-0.000435]\n",
            " 13%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | 13/100 [00:00<00:04, 19.61it/s, grad_norm=1.6e-7, d_parameter=0, d_loss=4.61e-10]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train auc: 0.7421426177024841\n",
            "test auc: 0.738603949546814\n",
            "train cttcorr: 0.9999991365943184\n",
            "test cttcorr: 0.9968088672331126\n"
          ]
        }
      ],
      "source": [
        "# Amortized Difficulty: fit item difficulties as a function of item embeddings (if available)\n",
        "# and free parameter otherwise, for each scenario and for the combined data.\n",
        "all_scenarios = results.columns.get_level_values(\"scenario\").unique().tolist() + [\"combined_data\"]\n",
        "# combined_data is for global simple rasch model\n",
        "\n",
        "for scenario in all_scenarios:\n",
        "    print(f\"\\n{scenario}\")\n",
        "    if scenario != \"combined_data\":\n",
        "        # Select only items belonging to this scenario\n",
        "        mask = (results.columns.get_level_values(\"scenario\") == scenario)\n",
        "        data_scenario = data_with0[:, mask]\n",
        "        data_idtor_scenario = data_idtor[:, mask]\n",
        "        embeds_scenario = [emb for emb, m in zip(embeds, mask) if m]\n",
        "    else:\n",
        "        # Use all items for combined data\n",
        "        data_scenario = data_with0\n",
        "        data_idtor_scenario = data_idtor\n",
        "        test_idtor_scenario = test_idtor\n",
        "        embeds_scenario = embeds\n",
        "    n_items_scenario = data_scenario.shape[1]\n",
        "    \n",
        "    # Identify which items have embedding features\n",
        "    has_feature = [0.0 if item is None else 1.0 for item in embeds_scenario]\n",
        "    has_feature = torch.tensor(has_feature, dtype=torch.float, device=device)\n",
        "    # Randomly assign 80% of items with features to train, rest to test\n",
        "    has_feature_train = torch.bernoulli(has_feature * 0.8).int()\n",
        "    has_feature_test = (has_feature - has_feature_train).int()\n",
        "\n",
        "    # Build feature matrix: zero vector for items without embedding, embedding vector otherwise\n",
        "    features = [[0] * embed_dim if i is None else i for i in embeds_scenario]\n",
        "    features = torch.tensor(features, dtype=torch.float, device=device)\n",
        "    # Build train/test indicator matrices for scenario\n",
        "    train_idtor_scenario = has_feature_train[None, :].repeat(n_test_takers, 1) * data_idtor_scenario\n",
        "    test_idtor_scenario = has_feature_test[None, :].repeat(n_test_takers, 1) * data_idtor_scenario\n",
        "    \n",
        "    B = 50000  # Batch size for optimization\n",
        "    thetas_nuisance = torch.randn(150, n_test_takers, device=device)  # Nuisance thetas for marginalization\n",
        "    w = torch.randn(embed_dim, requires_grad=True, device=device)      # Linear weights for embedding\n",
        "    b = torch.randn(1, requires_grad=True, device=device)              # Bias for embedding\n",
        "    z_free = torch.zeros(n_items_scenario, requires_grad=True, device=device)  # Free item difficulties for items w/o embedding\n",
        "    optim_z = LBFGS([z_free, w, b], lr=0.1, max_iter=20, history_size=10, line_search_fn=\"strong_wolfe\")\n",
        "    \n",
        "    def closure_z():\n",
        "        # Randomly sample a batch of items for optimization\n",
        "        idx = torch.randperm(n_items_scenario)[:B]\n",
        "        data_batch = data_scenario[:, idx]\n",
        "        train_idtor_batch = train_idtor_scenario[:, idx]\n",
        "        features_batch = features[idx]\n",
        "        has_feature_train_batch = has_feature_train[idx]\n",
        "        z_free_batch = z_free[idx]\n",
        "        \n",
        "        optim_z.zero_grad()\n",
        "        # For items with embedding: z = features@w + b; else: z = z_free\n",
        "        z = z_free_batch * (1 - has_feature_train_batch) + (features_batch@w + b) * has_feature_train_batch\n",
        "        probs = torch.sigmoid(thetas_nuisance[:, :, None] + z[None, None, :])\n",
        "        loss = -(Bernoulli(probs=probs).log_prob(data_batch)*train_idtor_batch).mean()\n",
        "        loss.backward()\n",
        "        return loss\n",
        "    \n",
        "    # Optimize item difficulties (z) and embedding weights (w, b)\n",
        "    z_free, w, b = trainer(\n",
        "        [z_free, w, b],\n",
        "        optim_z,\n",
        "        closure_z,\n",
        "        parameter_names=[\"z_free\", \"w\", \"b\"],\n",
        "        log_json_path=os.path.join(\"../result/logs\", f\"amortized_{scenario}_z_w_b.jsonl\")\n",
        "    )\n",
        "    # Compute final item difficulties for all items\n",
        "    z = z_free * (1 - has_feature) + (features@w + b) * has_feature\n",
        "    z = z.detach()\n",
        "\n",
        "    # Fit person abilities (theta) for this scenario\n",
        "    thetas = torch.randn(n_test_takers, requires_grad=True, device=device)\n",
        "    optim_theta = LBFGS([thetas], lr=0.1, max_iter=20, history_size=10, line_search_fn=\"strong_wolfe\")\n",
        "    def closure_theta():\n",
        "        optim_theta.zero_grad()\n",
        "        probs = torch.sigmoid(thetas[:, None] + z[None, :])\n",
        "        loss = -(Bernoulli(probs=probs).log_prob(data_scenario)*train_idtor_scenario).mean()\n",
        "        loss.backward()\n",
        "        return loss\n",
        "    thetas = trainer([thetas], optim_theta, closure_theta, parameter_names=[\"theta\"], log_json_path=os.path.join(\"../result/logs\", f\"amortized_{scenario}_theta.jsonl\"))[0]\n",
        "\n",
        "    # Compute predicted probabilities\n",
        "    probs = torch.sigmoid(thetas[:, None] + z[None, :])\n",
        "    \n",
        "    # Evaluate AUC on train and test sets\n",
        "    train_auc, test_auc = compute_auc(probs, data_scenario, train_idtor_scenario, test_idtor_scenario)\n",
        "    metric_results[scenario][\"amortized_train_auc\"] = train_auc.item()\n",
        "    metric_results[scenario][\"amortized_test_auc\"] = test_auc.item()\n",
        "\n",
        "    # Evaluate CTT correlation on train and test sets\n",
        "    train_cttcorr, test_cttcorr = compute_cttcorr(probs, data_scenario, train_idtor_scenario, test_idtor_scenario)\n",
        "    metric_results[scenario][\"amortized_train_cttcorr\"] = train_cttcorr.item()\n",
        "    metric_results[scenario][\"amortized_test_cttcorr\"] = test_cttcorr.item()\n",
        "    \n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(f'../result/calibration_metric_results.json', 'w') as f:\n",
        "    json.dump(metric_results, f, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_corr_double(\n",
        "    data1_train,\n",
        "    data1_test,\n",
        "    data2_train,\n",
        "    data2_test,\n",
        "    plot_path,\n",
        "    metric_name,\n",
        "):\n",
        "    with plt.rc_context(bundles.icml2024(usetex=True, family=\"serif\")):\n",
        "        corr_train = pearsonr(data1_train, data2_train).statistic\n",
        "        corr_test  = pearsonr(data1_test,  data2_test ).statistic\n",
        "\n",
        "        plt.figure(figsize=(6, 6))\n",
        "        plt.scatter(data1_train, data2_train, color=\"blue\", label=\"Train\")\n",
        "        plt.scatter(data1_test,  data2_test, color=\"red\",  label=\"Test\")\n",
        "        plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"black\")\n",
        "\n",
        "        plt.xlabel(r\"Amortized Calibration\", fontsize=25)\n",
        "        plt.ylabel(r\"Traditional Calibration\", fontsize=25)\n",
        "        plt.xlim(0, 1)\n",
        "        plt.ylim(0, 1)\n",
        "\n",
        "        title = (\n",
        "            rf\"{metric_name}. \"\n",
        "            rf\"$\\rho_{{train}}$ = {corr_train:.2f}, \"\n",
        "            rf\"$\\rho_{{test}}$ = {corr_test:.2f}\"\n",
        "        )\n",
        "        plt.title(title, fontsize=25)\n",
        "\n",
        "        plt.tick_params(axis=\"both\", labelsize=16)\n",
        "        plt.legend(fontsize=16)\n",
        "        plt.savefig(plot_path, dpi=300, bbox_inches=\"tight\")\n",
        "    \n",
        "with open(\"../result/calibration_metric_results.json\", \"r\") as f:\n",
        "    metric_results = json.load(f)\n",
        "\n",
        "train_aucs = []\n",
        "test_aucs = []\n",
        "amor_train_aucs = []\n",
        "amor_test_aucs = []\n",
        "for scenario in metric_results:\n",
        "    train_aucs.append(metric_results[scenario][\"train_auc\"])\n",
        "    test_aucs.append(metric_results[scenario][\"test_auc\"])\n",
        "    amor_train_aucs.append(metric_results[scenario][\"amortized_train_auc\"])\n",
        "    amor_test_aucs.append(metric_results[scenario][\"amortized_test_auc\"])\n",
        "plot_corr_double(amor_train_aucs, amor_test_aucs, train_aucs, test_aucs, \"../result/amor_vs_trad_auc.png\", r\"AUC-ROC\")\n",
        "\n",
        "train_cttcorrs = []\n",
        "test_cttcorrs = []\n",
        "amor_train_cttcorrs = []\n",
        "amor_test_cttcorrs = []\n",
        "for scenario in metric_results:\n",
        "    train_cttcorrs.append(metric_results[scenario][\"train_cttcorr\"])\n",
        "    test_cttcorrs.append(metric_results[scenario][\"test_cttcorr\"])\n",
        "    amor_train_cttcorrs.append(metric_results[scenario][\"amortized_train_cttcorr\"])\n",
        "    amor_test_cttcorrs.append(metric_results[scenario][\"amortized_test_cttcorr\"])\n",
        "plot_corr_double(amor_train_cttcorrs, amor_test_cttcorrs, train_cttcorrs, test_cttcorrs, \"../result/amor_vs_trad_cttcorr.png\", r\"$\\theta$ Corr Avg\")"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "reeval",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
